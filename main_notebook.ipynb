{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRiA7pev2mIF"
      },
      "source": [
        "# I. Pre-Process\n",
        "## A. Format\n",
        "Pré-process des PNG\n",
        "\n",
        "- Standardiser l’image : lire le PNG, convertir en RGB (gérer l’alpha si RGBA), puis resize + crop (ou pad) pour obtenir exactement la taille attendue par le backbone.\n",
        "\n",
        "- Mettre au format “modèle” : convertir en float tensor, normaliser avec les mean/std du pré-entraînement (ImageNet / CLIP selon le backbone)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D4a6GzKY2qfh"
      },
      "outputs": [],
      "source": [
        "Path_train = \"/content/drive/MyDrive/jaguar-re-id(1)/train/train\"\n",
        "Path_test = \"/content/drive/MyDrive/jaguar-re-id(1)/test/test\"\n",
        "import os\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import torch\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels=None, transform=None, float_size=\"fp32\"):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.float_size = float_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        x = self.transform(img) if self.transform else img\n",
        "\n",
        "        if self.float_size == \"fp16\":\n",
        "            x = x.half()\n",
        "        elif self.float_size == \"bf16\":\n",
        "            x = x.to(torch.bfloat16)\n",
        "\n",
        "        if self.labels is None:\n",
        "            return x, os.path.basename(p)  # test: (tensor, filename)\n",
        "        return x, self.labels[idx]        # train/val: (tensor, class_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbJnTY_mukrD"
      },
      "source": [
        "# II. DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynY2HoyI2p1X"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9NcGhDK2tjCr"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def make_loaders(\n",
        "    train_dir,\n",
        "    test_dir,\n",
        "    img_size=224,\n",
        "    batch_size=64,\n",
        "    num_workers=2,\n",
        "    val_size=1.0,\n",
        "    n_folds=5,\n",
        "    fold=0,\n",
        "    seed=42,\n",
        "    train_csv=\"/content/drive/MyDrive/jaguar-re-id(1)/train.csv\",\n",
        "    test_csv=\"/content/drive/MyDrive/jaguar-re-id(1)/test.csv\",\n",
        "    img_col=\"filename\",          # <-- CHANGE\n",
        "    label_col=\"ground_truth\",    # <-- CHANGE\n",
        "    float_size=\"fp32\",\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225),\n",
        "    train_tf=None, test_tf=None,\n",
        "):\n",
        "    def to_path(name, root):\n",
        "        name = str(name)\n",
        "        if not name.lower().endswith(\".png\"):\n",
        "            name += \".png\"\n",
        "        return name if os.path.isabs(name) else os.path.join(root, name)\n",
        "\n",
        "    # ---- TRAIN (from train.csv)\n",
        "    df_tr = pd.read_csv(train_csv)\n",
        "    train_paths = [to_path(n, train_dir) for n in df_tr[img_col].tolist()]\n",
        "    raw_labels  = df_tr[label_col].astype(str).tolist()\n",
        "\n",
        "    # ---- TEST (optional, for later inference)\n",
        "    if test_csv is not None and os.path.exists(test_csv):\n",
        "        df_te = pd.read_csv(test_csv)\n",
        "        if \"query_image\" in df_te.columns and \"gallery_image\" in df_te.columns:\n",
        "            test_imgs = sorted(set(df_te[\"query_image\"]) | set(df_te[\"gallery_image\"]))\n",
        "            test_paths = [to_path(n, test_dir) for n in test_imgs]\n",
        "        elif img_col in df_te.columns:\n",
        "            test_paths = [to_path(n, test_dir) for n in df_te[img_col].tolist()]\n",
        "        else:\n",
        "            test_paths = sorted(glob.glob(os.path.join(test_dir, \"*.png\")))\n",
        "    else:\n",
        "        test_paths = sorted(glob.glob(os.path.join(test_dir, \"*.png\")))\n",
        "\n",
        "    # ---- encode labels\n",
        "    classes = sorted(set(raw_labels))\n",
        "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "    y = [class_to_idx[c] for c in raw_labels]\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    # ---- transforms (train aug, val/test no aug)\n",
        "    if train_tf is None:\n",
        "        # fallback: your existing ViT transforms (keep your old code here)\n",
        "      train_tf = T.Compose([\n",
        "          T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
        "          T.RandomHorizontalFlip(p=0.5),\n",
        "          T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "          T.ToTensor(),\n",
        "          T.Normalize(mean, std),\n",
        "          T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=\"random\"),\n",
        "      ])\n",
        "\n",
        "    if test_tf is None:\n",
        "        test_tf = T.Compose([\n",
        "            T.Resize(img_size),\n",
        "            T.CenterCrop(img_size),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean, std),\n",
        "        ])\n",
        "\n",
        "    # ---- K-fold split\n",
        "    splitter = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed) if num_classes > 1 else \\\n",
        "               KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "\n",
        "    idx_all = list(range(len(train_paths)))\n",
        "    tr_idx, va_idx = list(splitter.split(idx_all, y if num_classes > 1 else None))[fold]\n",
        "\n",
        "    if val_size < 1.0:\n",
        "        g = torch.Generator().manual_seed(seed + fold)\n",
        "        va_idx = torch.tensor(va_idx)[torch.randperm(len(va_idx), generator=g)].tolist()\n",
        "        k = max(1, int(len(va_idx) * val_size))\n",
        "        va_keep, va_rest = va_idx[:k], va_idx[k:]\n",
        "        tr_idx = list(tr_idx) + list(va_rest)\n",
        "        va_idx = va_keep\n",
        "\n",
        "    tr_paths  = [train_paths[i] for i in tr_idx]\n",
        "    va_paths  = [train_paths[i] for i in va_idx]\n",
        "    tr_labels = [y[i] for i in tr_idx]\n",
        "    va_labels = [y[i] for i in va_idx]\n",
        "\n",
        "    train_ds = ImageDataset(tr_paths, tr_labels, transform=train_tf, float_size=float_size)\n",
        "    val_ds   = ImageDataset(va_paths, va_labels, transform=test_tf,  float_size=float_size)\n",
        "    test_ds  = ImageDataset(test_paths, None,     transform=test_tf,  float_size=float_size)\n",
        "    train_eval_ds = ImageDataset(tr_paths, tr_labels, transform=test_tf, float_size=float_size)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    train_eval_loader = DataLoader(train_eval_ds, batch_size=batch_size, shuffle=False,num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    meta = {\n",
        "        \"N_train\": len(train_ds),\n",
        "        \"N_val\": len(val_ds),\n",
        "        \"N_test\": len(test_ds),\n",
        "        \"num_classes\": num_classes,\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "    }\n",
        "    return train_loader, train_eval_loader, val_loader, test_loader, meta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1mmkwLaK763",
        "outputId": "508b8742-86cf-49b1-c557-5463537af42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA H100 80GB HBM3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"CPU runtime\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pwn7rmcmMrs8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZrzs0Wqun3B"
      },
      "source": [
        "# III. Model\n",
        "## A. Backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PyaeZCBvTcU"
      },
      "source": [
        "Backbone: Swin-L or OpenCLIP ViT-L/H, plus one strong CNN for ensemble diversity\n",
        "\n",
        "Head: GeM → Linear/BN/PReLU → L2-norm\n",
        "\n",
        "Loss: Sub-center ArcFace + dynamic margin\n",
        "\n",
        "Training: progressive resize, class-imbalance sampling, heavy aug\n",
        "\n",
        "Inference: cosine kNN + DBA/re-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVZH2uwuuubq"
      },
      "source": [
        "## B. Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Me4iIUAPDyRM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTBaseBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = ViT_B_16_Weights.IMAGENET1K_V1 if pretrained else None # 85M params\n",
        "        m = vit_b_16(weights=weights)\n",
        "        self.backbone = m\n",
        "        self.out_dim = m.heads.head.in_features\n",
        "        self.backbone.heads = nn.Identity()  # remove classifier -> returns (B, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)  # (B, D)\n",
        "        x = torch.nn.functional.normalize(x, dim=1)\n",
        "        return x\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3.0, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.p = nn.Parameter(torch.tensor(p))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):  # (B,C,H,W)\n",
        "        x = x.clamp(min=self.eps).pow(self.p)\n",
        "        x = F.avg_pool2d(x, (x.size(-2), x.size(-1)))\n",
        "        x = x.pow(1.0 / self.p)\n",
        "        return x.squeeze(-1).squeeze(-1)  # (B,C)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    GeM → Linear/BN/PReLU → L2-norm\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, out_dim=None):\n",
        "        super().__init__()\n",
        "        out_dim = d_model if out_dim is None else out_dim\n",
        "        self.gem = GeM()\n",
        "        self.fc  = nn.Linear(d_model, out_dim, bias=False)\n",
        "        self.bn  = nn.BatchNorm1d(out_dim)\n",
        "        self.act = nn.PReLU(out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x can be (B,C,H,W) or already pooled (B,C)\n",
        "        if x.dim() == 4:\n",
        "            x = self.gem(x)\n",
        "        x = self.act(self.bn(self.fc(x)))\n",
        "        return F.normalize(x, dim=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrkMPmPJuy3z"
      },
      "source": [
        "## C. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_HNzLMPGX1Kw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torchvision.models.vision_transformer import interpolate_embeddings\n",
        "import timm\n",
        "from timm.data import resolve_model_data_config, create_transform\n",
        "\n",
        "\n",
        "# ---- ArcFace (minimal) + dynamic margin handled in train loop\n",
        "class ArcFace(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes, s=30.0):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty(num_classes, in_dim))\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, x, y, m):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        W = F.normalize(self.W, dim=1)\n",
        "        cos = F.linear(x, W).clamp(-1 + 1e-7, 1 - 1e-7)  # (B, num_classes)\n",
        "\n",
        "        sin = torch.sqrt(1.0 - cos * cos)\n",
        "        cos_m, sin_m = torch.cos(torch.tensor(m, device=x.device)), torch.sin(torch.tensor(m, device=x.device))\n",
        "        phi = cos * cos_m - sin * sin_m\n",
        "\n",
        "        onehot = torch.zeros_like(cos)\n",
        "        onehot.scatter_(1, y.view(-1, 1), 1.0)\n",
        "        logits = (onehot * phi) + ((1.0 - onehot) * cos)\n",
        "        return logits * self.s\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_emb(loader, vit, head, device):\n",
        "    embs, ys = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        e = head(vit(x))\n",
        "        embs.append(e.cpu())\n",
        "        ys.append(y)\n",
        "    return torch.cat(embs), torch.cat(ys)\n",
        "\n",
        "@torch.no_grad()\n",
        "def dba(gallery_emb, k=10):\n",
        "    # gallery_emb must be L2-normalized\n",
        "    sim = gallery_emb @ gallery_emb.T\n",
        "    idx = sim.topk(k + 1, dim=1).indices[:, 1:]      # skip self\n",
        "    neigh = gallery_emb[idx]                          # (N, k, D)\n",
        "    out = (gallery_emb.unsqueeze(1) + neigh).mean(1)  # (N, D)\n",
        "    return F.normalize(out, dim=1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def knn_acc(query_emb, query_y, gallery_emb, gallery_y, k=1):\n",
        "    sim = query_emb @ gallery_emb.T\n",
        "    nn_idx = sim.topk(k, dim=1).indices              # (Nq, k)\n",
        "    pred = gallery_y[nn_idx[:, 0]]                   # top-1\n",
        "    return (pred == query_y).float().mean().item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gBXHS2w7xK6G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import timm\n",
        "from timm.data import resolve_model_data_config, create_transform\n",
        "\n",
        "def build_timm_transforms(model, img_size, is_train):\n",
        "    cfg = resolve_model_data_config(model)\n",
        "    cfg = dict(cfg)\n",
        "    cfg[\"input_size\"] = (3, img_size, img_size)   # force stage size\n",
        "    tf = create_transform(**cfg, is_training=is_train)\n",
        "    return tf, cfg\n",
        "def average_precision_from_scores(scores: np.ndarray, rel: np.ndarray) -> float:\n",
        "    order = scores.argsort()[::-1]\n",
        "    rel_sorted = np.asarray(rel, dtype=bool)[order]\n",
        "    pos = np.flatnonzero(rel_sorted)          # ranks (0-based) where rel=1\n",
        "    if pos.size == 0:\n",
        "        return 0.0\n",
        "    return float((np.arange(1, pos.size + 1) / (pos + 1)).mean())\n",
        "\n",
        "\n",
        "def identity_balanced_map(\n",
        "    query_emb: torch.Tensor,\n",
        "    query_y: torch.Tensor,\n",
        "    gallery_emb: torch.Tensor,\n",
        "    gallery_y: torch.Tensor,\n",
        "    query_names=None,\n",
        "    gallery_names=None,\n",
        "    remove_self=True,\n",
        ") -> float:\n",
        "    sim = (query_emb @ gallery_emb.T).detach().cpu().numpy()   # (Nq, Ng)\n",
        "    qy  = query_y.detach().cpu().numpy()\n",
        "    gy  = gallery_y.detach().cpu().numpy()\n",
        "\n",
        "    # precompute relevance masks per identity in the gallery\n",
        "    masks = {i: (gy == i) for i in np.unique(qy)}\n",
        "\n",
        "    name_to_gidx = None\n",
        "    if remove_self and query_names is not None and gallery_names is not None:\n",
        "        name_to_gidx = {n: j for j, n in enumerate(gallery_names)}\n",
        "\n",
        "    sum_ap, cnt = {}, {}\n",
        "    for i, ident in enumerate(qy):\n",
        "        order = sim[i].argsort()[::-1]\n",
        "        rel_sorted = masks[ident][order]\n",
        "\n",
        "        # remove self-match if query image also exists in gallery\n",
        "        if name_to_gidx is not None:\n",
        "            j = name_to_gidx.get(query_names[i], None)\n",
        "            if j is not None:\n",
        "                rel_sorted = rel_sorted & (order != j)\n",
        "\n",
        "        pos = np.flatnonzero(rel_sorted)\n",
        "        ap = 0.0 if pos.size == 0 else float((np.arange(1, pos.size + 1) / (pos + 1)).mean())\n",
        "\n",
        "        sum_ap[ident] = sum_ap.get(ident, 0.0) + ap\n",
        "        cnt[ident]    = cnt.get(ident, 0) + 1\n",
        "\n",
        "    return float(np.mean([sum_ap[k] / cnt[k] for k in sum_ap])) if sum_ap else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MOajTYlBxhoo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_emb_and_names(loader, vit, head, device):\n",
        "    embs, ys, names = [], [], []\n",
        "    ptr = 0  # used only when loader returns labels and dataset has paths\n",
        "\n",
        "    for x, y_or_name in loader:\n",
        "        bs = x.size(0)\n",
        "        x = x.to(device)\n",
        "        embs.append(head(vit(x)).cpu())\n",
        "\n",
        "        if isinstance(y_or_name[0], str):\n",
        "            names.extend(list(y_or_name))\n",
        "        else:\n",
        "            ys.append(y_or_name.cpu())\n",
        "            # recover filenames if possible (val/test_tf loaders are usually not shuffled)\n",
        "            if hasattr(loader.dataset, \"paths\"):\n",
        "                batch_paths = loader.dataset.paths[ptr:ptr+bs]\n",
        "                names.extend([os.path.basename(p) for p in batch_paths])\n",
        "                ptr += bs\n",
        "\n",
        "    embs = torch.cat(embs)\n",
        "    ys = torch.cat(ys) if len(ys) else None\n",
        "    return embs, ys, names\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU memory testing"
      ],
      "metadata": {
        "id": "RBbcMpRTKrsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch, torch.nn.functional as F\n",
        "\n",
        "def enable_grad_ckpt(model, enabled=True):\n",
        "    if hasattr(model, \"set_grad_checkpointing\"):\n",
        "        model.set_grad_checkpointing(enabled)\n",
        "        return True\n",
        "    if hasattr(model, \"grad_checkpointing\"):\n",
        "        model.grad_checkpointing = enabled\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _set_bn_eval(m):\n",
        "    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        m.eval()\n",
        "\n",
        "def one_train_step_peak_gb(backbone, head, arc, img_size, batch_size, num_classes, device=\"cuda\", amp=True):\n",
        "    backbone.train(); head.train(); arc.train()\n",
        "\n",
        "    # BN-safe: if batch_size==1, run BN layers in eval (head only)\n",
        "    if batch_size == 1:\n",
        "        head.apply(_set_bn_eval)\n",
        "\n",
        "    amp = amp and device.startswith(\"cuda\")\n",
        "    opt = torch.optim.AdamW(\n",
        "        list(backbone.parameters()) + list(head.parameters()) + list(arc.parameters()),\n",
        "        lr=1e-4\n",
        "    )\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=amp)\n",
        "\n",
        "    if device.startswith(\"cuda\"):\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
        "    y = torch.randint(0, num_classes, (batch_size,), device=device)\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    with torch.amp.autocast(device_type=\"cuda\", enabled=amp):\n",
        "        emb = head(backbone(x))\n",
        "        logits = arc(emb, y, 0.5)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(opt)\n",
        "    scaler.update()\n",
        "\n",
        "    if device.startswith(\"cuda\"):\n",
        "        return torch.cuda.max_memory_allocated() / 1024**3\n",
        "    return 0.0\n",
        "\n",
        "def find_max_batch(backbone, head, arc, img_size, num_classes, device=\"cuda\", amp=True,\n",
        "                   candidates=(2,4,8,16,24,32,48,64,96,128)):\n",
        "    ok_bs, ok_peak = None, None\n",
        "    for bs in candidates:\n",
        "        try:\n",
        "            peak = one_train_step_peak_gb(backbone, head, arc, img_size, bs, num_classes, device=device, amp=amp)\n",
        "            ok_bs, ok_peak = bs, peak\n",
        "            print(f\"OK  bs={bs:>3} | peak_alloc={peak:.2f} GB\")\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            print(f\"OOM bs={bs:>3}\")\n",
        "            if device.startswith(\"cuda\"):\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            break\n",
        "    return ok_bs, ok_peak\n"
      ],
      "metadata": {
        "id": "HNWrvkAQKt97"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "def train_fold_SWIN(train_dir, test_dir, fold=0, n_folds=5,\n",
        "                    epochs_stage_1=5, epochs_stage_2=2,\n",
        "                    lr_stage_1=1e-3, lr_stage_2=4e-4,\n",
        "                    out_dir=\"/content/drive/MyDrive/jaguar-re-id(1)/ckpts/\"):\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    stages = [(384, epochs_stage_1, lr_stage_1), (448, epochs_stage_2, lr_stage_2)]\n",
        "    ckpt0 = os.path.join(out_dir, f\"mega_fold{fold}_stage0.pth\")\n",
        "    ckptb = os.path.join(out_dir, f\"mega_fold{fold}_best.pth\")\n",
        "\n",
        "    best = -1.0\n",
        "    best_meta = None\n",
        "\n",
        "    for si, (img_size, epochs, lr) in enumerate(stages):\n",
        "        backbone = timm.create_model(\"hf-hub:BVRA/MegaDescriptor-L-384\", pretrained=(si == 0)).to(device)\n",
        "\n",
        "        train_tf, _ = build_timm_transforms(backbone, img_size, is_train=True)\n",
        "        test_tf,  _ = build_timm_transforms(backbone, img_size, is_train=False)\n",
        "\n",
        "        train_loader, train_eval_loader, val_loader, _, meta = make_loaders(\n",
        "            train_dir=train_dir, test_dir=test_dir,\n",
        "            img_size=img_size, n_folds=n_folds, fold=fold,\n",
        "            train_tf=train_tf, test_tf=test_tf\n",
        "        )\n",
        "        num_classes = meta[\"num_classes\"]\n",
        "\n",
        "        head = Head(d_model=1536).to(device)\n",
        "        arc  = ArcFace(in_dim=1536, num_classes=num_classes).to(device)\n",
        "\n",
        "        if si > 0:\n",
        "            s = torch.load(ckpt0, map_location=\"cpu\")\n",
        "            backbone.load_state_dict(s[\"backbone\"], strict=True)\n",
        "            head.load_state_dict(s[\"head\"], strict=True)\n",
        "            arc.load_state_dict(s[\"arc\"], strict=True)\n",
        "\n",
        "        opt = torch.optim.Adam([\n",
        "            {\"params\": backbone.parameters(), \"lr\": lr * 0.1},\n",
        "            {\"params\": head.parameters(),     \"lr\": lr},\n",
        "            {\"params\": arc.parameters(),      \"lr\": lr},\n",
        "        ])\n",
        "        use_amp = (device == \"cuda\")\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "        m = 0.5\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            backbone.train(); head.train(); arc.train()\n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "                    emb = head(backbone(x))\n",
        "                    loss = F.cross_entropy(arc(emb, y, m), y)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "\n",
        "            backbone.eval(); head.eval(); arc.eval()\n",
        "            tr_emb, tr_y, _ = get_emb_and_names(train_eval_loader, backbone, head, device)\n",
        "            va_emb, va_y, _ = get_emb_and_names(val_loader,       backbone, head, device)\n",
        "            score = identity_balanced_map(va_emb, va_y, tr_emb, tr_y, remove_self=False)\n",
        "\n",
        "            print(f\"[fold {fold}] [stage {img_size}] ep {ep+1}/{epochs} ibmAP={score:.4f}\")\n",
        "\n",
        "            if score > best:\n",
        "                best, best_meta = score, meta\n",
        "                torch.save(\n",
        "                    {\"img_size\": img_size, \"backbone\": backbone.state_dict(), \"head\": head.state_dict(), \"arc\": arc.state_dict()},\n",
        "                    ckptb\n",
        "                )\n",
        "\n",
        "        if si == 0:\n",
        "            torch.save(\n",
        "                {\"img_size\": img_size, \"backbone\": backbone.state_dict(), \"head\": head.state_dict(), \"arc\": arc.state_dict()},\n",
        "                ckpt0\n",
        "            )\n",
        "\n",
        "    return ckptb, best, best_meta\n",
        "\n",
        "\n",
        "def run_cv_SWIN(train_dir, test_dir, n_folds=5,\n",
        "                epochs_stage_1=5, epochs_stage_2=2,\n",
        "                lr_stage_1=1e-3, lr_stage_2=4e-4,\n",
        "                out_dir=\"/content/drive/MyDrive/jaguar-re-id(1)/ckpts/\"):\n",
        "\n",
        "    ckpts, scores = [], []\n",
        "    for fold in range(n_folds):\n",
        "        ckpt, score, _ = train_fold_SWIN(\n",
        "            train_dir, test_dir, fold=fold, n_folds=n_folds,\n",
        "            epochs_stage_1=epochs_stage_1, epochs_stage_2=epochs_stage_2,\n",
        "            lr_stage_1=lr_stage_1, lr_stage_2=lr_stage_2,\n",
        "            out_dir=out_dir\n",
        "        )\n",
        "        ckpts.append(ckpt); scores.append(score)\n",
        "    return ckpts, scores\n"
      ],
      "metadata": {
        "id": "dDxELNBqIobf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "out_dir = \"/content/drive/MyDrive/jaguar-re-id(1)/ckpts_MegaDescriptor/\"\n",
        "\n",
        "ckpts, scores = run_cv_SWIN(Path_train, Path_test, n_folds=2, epochs_stage_1=12, epochs_stage_2=2,lr_stage_1=1e-3, lr_stage_2=4e-4 ,out_dir=out_dir)\n",
        "\n",
        "print(\"ckpts:\", ckpts)\n",
        "print(\"scores:\", scores)\n",
        "print(\"mean:\", sum(scores)/len(scores))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UxtYK3VJG52",
        "outputId": "bfd18abc-9bab-4392-b2f9-5340cdce1063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1814874877.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "/tmp/ipython-input-1814874877.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] [stage 384] ep 1/12 ibmAP=0.4948\n",
            "[fold 0] [stage 384] ep 2/12 ibmAP=0.6220\n",
            "[fold 0] [stage 384] ep 3/12 ibmAP=0.7284\n",
            "[fold 0] [stage 384] ep 4/12 ibmAP=0.7680\n",
            "[fold 0] [stage 384] ep 5/12 ibmAP=0.7935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn7tzTe_xKts"
      },
      "outputs": [],
      "source": [
        "# ---- Training function: progressive resize + pos-emb interpolation (1 fold)\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def train_fold(train_dir, test_dir, fold=0, n_folds=5,\n",
        "               epochs_224=2, epochs_384=1, lr_224=3e-4, lr_384=1e-4,\n",
        "               out_dir=\"/content/drive/MyDrive/jaguar-re-id(1)/ckpts/\"):\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    pre = weights.transforms()\n",
        "    mean = pre.mean if hasattr(pre, \"mean\") else (0.485, 0.456, 0.406)\n",
        "    std  = pre.std  if hasattr(pre, \"std\")  else (0.229, 0.224, 0.225)\n",
        "\n",
        "    ckpt_stage0 = os.path.join(out_dir, f\"vit_fold{fold}_stage224.pth\")\n",
        "    ckpt_best   = os.path.join(out_dir, f\"vit_fold{fold}_best.pth\")\n",
        "\n",
        "    stages = [(224, epochs_224, lr_224), (384, epochs_384, lr_384)]\n",
        "\n",
        "    best_score = -1.0\n",
        "    best_meta = None\n",
        "\n",
        "    for stage_i, (img_size, epochs, lr) in enumerate(stages):\n",
        "        train_loader, train_eval_loader, val_loader, _, meta = make_loaders(\n",
        "            train_dir=train_dir, test_dir=test_dir,\n",
        "            img_size=img_size, n_folds=n_folds, fold=fold,\n",
        "            mean=mean, std=std\n",
        "        )\n",
        "        num_classes = meta[\"num_classes\"]\n",
        "\n",
        "        if stage_i == 0:\n",
        "            vit = vit_b_16(weights=weights, image_size=img_size)\n",
        "            vit.heads = nn.Identity()\n",
        "            head = Head(d_model=768).to(device)\n",
        "            arc  = ArcFace(in_dim=768, num_classes=num_classes).to(device)\n",
        "        else:\n",
        "            vit = vit_b_16(weights=None, image_size=img_size)\n",
        "            vit.heads = nn.Identity()\n",
        "            head = Head(d_model=768).to(device)\n",
        "            arc  = ArcFace(in_dim=768, num_classes=num_classes).to(device)\n",
        "\n",
        "            state = torch.load(ckpt_stage0, map_location=\"cpu\")\n",
        "            vit_state = interpolate_embeddings(\n",
        "                image_size=img_size, patch_size=16,\n",
        "                model_state=state[\"vit\"], interpolation_mode=\"bicubic\"\n",
        "            )\n",
        "            vit.load_state_dict(vit_state, strict=False)\n",
        "            head.load_state_dict(state[\"head\"])\n",
        "            arc.load_state_dict(state[\"arc\"])\n",
        "\n",
        "        vit = vit.to(device)\n",
        "        #for p in vit.parameters(): p.requires_grad = False\n",
        "        #vit.eval()\n",
        "\n",
        "        vit_lr = lr * 0.1  # <-- smaller lr for ViT (example: 10x smaller)\n",
        "\n",
        "        opt = torch.optim.Adam([\n",
        "            {\"params\": vit.parameters(),  \"lr\": vit_lr},\n",
        "            {\"params\": head.parameters(), \"lr\": lr},\n",
        "            {\"params\": arc.parameters(),  \"lr\": lr},\n",
        "        ])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            head.train(); arc.train();vit.train()\n",
        "\n",
        "            m_start, m_end = 0.0, 0.5\n",
        "            m = m_start if epochs == 1 else (m_start + (m_end - m_start) * epoch / (epochs - 1))\n",
        "\n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                opt.zero_grad()\n",
        "                f = vit(x)\n",
        "                emb = head(f)\n",
        "                logits = arc(emb, y, m)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            # ---- VALIDATION (your ib-mAP eval)\n",
        "            vit.eval()\n",
        "            head.eval()\n",
        "            arc.eval()\n",
        "\n",
        "            # IMPORTANT: use a non-aug train loader for embeddings (shuffle=False, test_tf)\n",
        "            tr_emb, tr_y, tr_names = get_emb_and_names(train_eval_loader, vit, head, device)\n",
        "            va_emb, va_y, va_names = get_emb_and_names(val_loader,       vit, head, device)\n",
        "\n",
        "            score_map = identity_balanced_map(\n",
        "                query_emb=va_emb, query_y=va_y,\n",
        "                gallery_emb=tr_emb, gallery_y=tr_y,\n",
        "                remove_self=False,\n",
        "            )\n",
        "\n",
        "            print(f\"[fold {fold}] [stage {img_size}] epoch {epoch+1}/{epochs}  margin={m:.3f}  val_ibmAP={score_map:.4f}\")\n",
        "\n",
        "            if score_map > best_score:\n",
        "                best_score = score_map\n",
        "                best_meta = meta\n",
        "                torch.save(\n",
        "                    {\"vit\": vit.state_dict(), \"head\": head.state_dict(), \"arc\": arc.state_dict()},\n",
        "                    ckpt_best\n",
        "                )\n",
        "\n",
        "        # save stage-0 checkpoint for stage-1 init\n",
        "        if stage_i == 0:\n",
        "            torch.save(\n",
        "                {\"vit\": vit.state_dict(), \"head\": head.state_dict(), \"arc\": arc.state_dict()},\n",
        "                ckpt_stage0\n",
        "            )\n",
        "\n",
        "    return ckpt_best, best_score, best_meta\n",
        "\n",
        "\n",
        "\n",
        "def run_cv(train_dir, test_dir, n_folds=5,\n",
        "           epochs_224=2, epochs_384=1, lr_224=3e-4, lr_384=1e-4,\n",
        "           out_dir=\"/content/drive/MyDrive/jaguar-re-id(1)/ckpts/\"):\n",
        "\n",
        "    ckpts, scores = [], []\n",
        "    for fold in range(n_folds):\n",
        "        ckpt, score, _ = train_fold(\n",
        "            train_dir, test_dir,\n",
        "            fold=fold, n_folds=n_folds,\n",
        "            epochs_224=epochs_224, epochs_384=epochs_384,\n",
        "            lr_224=lr_224, lr_384=lr_384,\n",
        "            out_dir=out_dir\n",
        "        )\n",
        "        ckpts.append(ckpt)\n",
        "        scores.append(score)\n",
        "    return ckpts, scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNu5mSrVcnD",
        "outputId": "f7d3c58a-a898-4a78-f059-f899315ca586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330M/330M [00:02<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] [stage 224] epoch 1/5  margin=0.000  val_ibmAP=0.4623\n",
            "[fold 0] [stage 224] epoch 2/5  margin=0.125  val_ibmAP=0.6246\n",
            "[fold 0] [stage 224] epoch 3/5  margin=0.250  val_ibmAP=0.7365\n",
            "[fold 0] [stage 224] epoch 4/5  margin=0.375  val_ibmAP=0.7971\n",
            "[fold 0] [stage 224] epoch 5/5  margin=0.500  val_ibmAP=0.8297\n"
          ]
        }
      ],
      "source": [
        "\n",
        "out_dir = \"/content/drive/MyDrive/jaguar-re-id(1)/ckpts_MegaDescriptor/\"\n",
        "\n",
        "ckpts, scores = run_cv(Path_train, Path_test, n_folds=2, epochs_384=12, epochs_512=2, out_dir=out_dir)\n",
        "\n",
        "print(\"ckpts:\", ckpts)\n",
        "print(\"scores:\", scores)\n",
        "print(\"mean:\", sum(scores)/len(scores))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q74hy2jRxqA6"
      },
      "source": [
        "## D.Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzYyarReggXF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16\n",
        "from torchvision.models.vision_transformer import interpolate_embeddings\n",
        "\n",
        "def load_vit_head(ckpt_path, device, force_img_size=None):\n",
        "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    vit_state = state[\"vit\"]\n",
        "\n",
        "    # ---- infer checkpoint img_size from pos_embedding length\n",
        "    # pos_embedding shape: (1, 1 + (H/16)*(W/16), 768)\n",
        "    n_tokens = vit_state[\"encoder.pos_embedding\"].shape[1]\n",
        "    grid = int(round(math.sqrt(n_tokens - 1)))   # e.g. 14 for 224, 24 for 384\n",
        "    ckpt_img_size = grid * 16\n",
        "\n",
        "    img_size = ckpt_img_size if force_img_size is None else int(force_img_size)\n",
        "\n",
        "    vit = vit_b_16(weights=None, image_size=img_size)\n",
        "    vit.heads = nn.Identity()\n",
        "    head = Head(d_model=768)\n",
        "\n",
        "    # ---- if you force a different img_size, interpolate pos embeddings\n",
        "    if img_size != ckpt_img_size:\n",
        "        vit_state = interpolate_embeddings(\n",
        "            image_size=img_size,\n",
        "            patch_size=16,\n",
        "            model_state=vit_state,\n",
        "            interpolation_mode=\"bicubic\"\n",
        "        )\n",
        "\n",
        "    vit.load_state_dict(vit_state, strict=True)\n",
        "    head.load_state_dict(state[\"head\"], strict=True)\n",
        "\n",
        "    vit.to(device).eval()\n",
        "    head.to(device).eval()\n",
        "    return vit, head, img_size\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_images(unique_filenames, test_dir, vit, head, device, img_size, mean, std, batch_size=32):\n",
        "    import torchvision.transforms as T\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    tf = T.Compose([\n",
        "        T.Resize(img_size),\n",
        "        T.CenterCrop(img_size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "    print(\"doing dataloader\")\n",
        "    paths = [os.path.join(test_dir, fn) for fn in unique_filenames]\n",
        "    ds = ImageDataset(paths, labels=None, transform=tf, float_size=\"fp32\")\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    emb_map = {}\n",
        "    for x, names in tqdm(loader):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "            e = head(vit(x))\n",
        "        e = e.float().cpu()\n",
        "        for name, vec in zip(names, e):\n",
        "            emb_map[name] = vec\n",
        "    return emb_map\n",
        "\n",
        "def make_submission(test_csv_path, test_dir, ckpt_path, out_path=\"submission.csv\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std  = (0.229, 0.224, 0.225)\n",
        "\n",
        "    df = pd.read_csv(test_csv_path)\n",
        "\n",
        "    # optional safety checks (recommended)\n",
        "    # ensures you match test.csv order and count\n",
        "    assert \"row_id\" in df.columns and \"query_image\" in df.columns and \"gallery_image\" in df.columns\n",
        "    assert len(df) == df[\"row_id\"].shape[0]\n",
        "\n",
        "    unique_imgs = sorted(set(df[\"query_image\"]) | set(df[\"gallery_image\"]))\n",
        "\n",
        "    vit, head, img_size = load_vit_head(ckpt_path, device, force_img_size=384)\n",
        "    print(\"doing embed image\")\n",
        "    emb_map = embed_images(unique_imgs, test_dir, vit, head, device, img_size, mean, std)\n",
        "\n",
        "    sims = []\n",
        "    for i,(q, g) in enumerate(zip(df[\"query_image\"].values, df[\"gallery_image\"].values)):\n",
        "        cos_sim = float((emb_map[q] * emb_map[g]).sum().item())  # in [-1, 1]\n",
        "        sim01 = (cos_sim + 1.0) * 0.5\n",
        "        if i % 100 == 0 :\n",
        "            print(f\"Scoring pairs: {i}/{len(df)} ({100*i/len(df):.1f}%)\")\n",
        "        sims.append(min(1.0, max(0.0, sim01)))  # clamp to [0,1]\n",
        "\n",
        "    sub = pd.DataFrame({\"row_id\": df[\"row_id\"].values, \"similarity\": sims})\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXWRu6Q2VcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b499b0-9eb6-4787-e60d-a932085e1d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doing embed image\n",
            "doing dataloader\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/12 [00:00<?, ?it/s]/tmp/ipython-input-552543354.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "100%|██████████| 12/12 [02:59<00:00, 14.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring pairs: 0/137270 (0.0%)\n",
            "Scoring pairs: 100/137270 (0.1%)\n",
            "Scoring pairs: 200/137270 (0.1%)\n",
            "Scoring pairs: 300/137270 (0.2%)\n",
            "Scoring pairs: 400/137270 (0.3%)\n",
            "Scoring pairs: 500/137270 (0.4%)\n",
            "Scoring pairs: 600/137270 (0.4%)\n",
            "Scoring pairs: 700/137270 (0.5%)\n",
            "Scoring pairs: 800/137270 (0.6%)\n",
            "Scoring pairs: 900/137270 (0.7%)\n",
            "Scoring pairs: 1000/137270 (0.7%)\n",
            "Scoring pairs: 1100/137270 (0.8%)\n",
            "Scoring pairs: 1200/137270 (0.9%)\n",
            "Scoring pairs: 1300/137270 (0.9%)\n",
            "Scoring pairs: 1400/137270 (1.0%)\n",
            "Scoring pairs: 1500/137270 (1.1%)\n",
            "Scoring pairs: 1600/137270 (1.2%)\n",
            "Scoring pairs: 1700/137270 (1.2%)\n",
            "Scoring pairs: 1800/137270 (1.3%)\n",
            "Scoring pairs: 1900/137270 (1.4%)\n",
            "Scoring pairs: 2000/137270 (1.5%)\n",
            "Scoring pairs: 2100/137270 (1.5%)\n",
            "Scoring pairs: 2200/137270 (1.6%)\n",
            "Scoring pairs: 2300/137270 (1.7%)\n",
            "Scoring pairs: 2400/137270 (1.7%)\n",
            "Scoring pairs: 2500/137270 (1.8%)\n",
            "Scoring pairs: 2600/137270 (1.9%)\n",
            "Scoring pairs: 2700/137270 (2.0%)\n",
            "Scoring pairs: 2800/137270 (2.0%)\n",
            "Scoring pairs: 2900/137270 (2.1%)\n",
            "Scoring pairs: 3000/137270 (2.2%)\n",
            "Scoring pairs: 3100/137270 (2.3%)\n",
            "Scoring pairs: 3200/137270 (2.3%)\n",
            "Scoring pairs: 3300/137270 (2.4%)\n",
            "Scoring pairs: 3400/137270 (2.5%)\n",
            "Scoring pairs: 3500/137270 (2.5%)\n",
            "Scoring pairs: 3600/137270 (2.6%)\n",
            "Scoring pairs: 3700/137270 (2.7%)\n",
            "Scoring pairs: 3800/137270 (2.8%)\n",
            "Scoring pairs: 3900/137270 (2.8%)\n",
            "Scoring pairs: 4000/137270 (2.9%)\n",
            "Scoring pairs: 4100/137270 (3.0%)\n",
            "Scoring pairs: 4200/137270 (3.1%)\n",
            "Scoring pairs: 4300/137270 (3.1%)\n",
            "Scoring pairs: 4400/137270 (3.2%)\n",
            "Scoring pairs: 4500/137270 (3.3%)\n",
            "Scoring pairs: 4600/137270 (3.4%)\n",
            "Scoring pairs: 4700/137270 (3.4%)\n",
            "Scoring pairs: 4800/137270 (3.5%)\n",
            "Scoring pairs: 4900/137270 (3.6%)\n",
            "Scoring pairs: 5000/137270 (3.6%)\n",
            "Scoring pairs: 5100/137270 (3.7%)\n",
            "Scoring pairs: 5200/137270 (3.8%)\n",
            "Scoring pairs: 5300/137270 (3.9%)\n",
            "Scoring pairs: 5400/137270 (3.9%)\n",
            "Scoring pairs: 5500/137270 (4.0%)\n",
            "Scoring pairs: 5600/137270 (4.1%)\n",
            "Scoring pairs: 5700/137270 (4.2%)\n",
            "Scoring pairs: 5800/137270 (4.2%)\n",
            "Scoring pairs: 5900/137270 (4.3%)\n",
            "Scoring pairs: 6000/137270 (4.4%)\n",
            "Scoring pairs: 6100/137270 (4.4%)\n",
            "Scoring pairs: 6200/137270 (4.5%)\n",
            "Scoring pairs: 6300/137270 (4.6%)\n",
            "Scoring pairs: 6400/137270 (4.7%)\n",
            "Scoring pairs: 6500/137270 (4.7%)\n",
            "Scoring pairs: 6600/137270 (4.8%)\n",
            "Scoring pairs: 6700/137270 (4.9%)\n",
            "Scoring pairs: 6800/137270 (5.0%)\n",
            "Scoring pairs: 6900/137270 (5.0%)\n",
            "Scoring pairs: 7000/137270 (5.1%)\n",
            "Scoring pairs: 7100/137270 (5.2%)\n",
            "Scoring pairs: 7200/137270 (5.2%)\n",
            "Scoring pairs: 7300/137270 (5.3%)\n",
            "Scoring pairs: 7400/137270 (5.4%)\n",
            "Scoring pairs: 7500/137270 (5.5%)\n",
            "Scoring pairs: 7600/137270 (5.5%)\n",
            "Scoring pairs: 7700/137270 (5.6%)\n",
            "Scoring pairs: 7800/137270 (5.7%)\n",
            "Scoring pairs: 7900/137270 (5.8%)\n",
            "Scoring pairs: 8000/137270 (5.8%)\n",
            "Scoring pairs: 8100/137270 (5.9%)\n",
            "Scoring pairs: 8200/137270 (6.0%)\n",
            "Scoring pairs: 8300/137270 (6.0%)\n",
            "Scoring pairs: 8400/137270 (6.1%)\n",
            "Scoring pairs: 8500/137270 (6.2%)\n",
            "Scoring pairs: 8600/137270 (6.3%)\n",
            "Scoring pairs: 8700/137270 (6.3%)\n",
            "Scoring pairs: 8800/137270 (6.4%)\n",
            "Scoring pairs: 8900/137270 (6.5%)\n",
            "Scoring pairs: 9000/137270 (6.6%)\n",
            "Scoring pairs: 9100/137270 (6.6%)\n",
            "Scoring pairs: 9200/137270 (6.7%)\n",
            "Scoring pairs: 9300/137270 (6.8%)\n",
            "Scoring pairs: 9400/137270 (6.8%)\n",
            "Scoring pairs: 9500/137270 (6.9%)\n",
            "Scoring pairs: 9600/137270 (7.0%)\n",
            "Scoring pairs: 9700/137270 (7.1%)\n",
            "Scoring pairs: 9800/137270 (7.1%)\n",
            "Scoring pairs: 9900/137270 (7.2%)\n",
            "Scoring pairs: 10000/137270 (7.3%)\n",
            "Scoring pairs: 10100/137270 (7.4%)\n",
            "Scoring pairs: 10200/137270 (7.4%)\n",
            "Scoring pairs: 10300/137270 (7.5%)\n",
            "Scoring pairs: 10400/137270 (7.6%)\n",
            "Scoring pairs: 10500/137270 (7.6%)\n",
            "Scoring pairs: 10600/137270 (7.7%)\n",
            "Scoring pairs: 10700/137270 (7.8%)\n",
            "Scoring pairs: 10800/137270 (7.9%)\n",
            "Scoring pairs: 10900/137270 (7.9%)\n",
            "Scoring pairs: 11000/137270 (8.0%)\n",
            "Scoring pairs: 11100/137270 (8.1%)\n",
            "Scoring pairs: 11200/137270 (8.2%)\n",
            "Scoring pairs: 11300/137270 (8.2%)\n",
            "Scoring pairs: 11400/137270 (8.3%)\n",
            "Scoring pairs: 11500/137270 (8.4%)\n",
            "Scoring pairs: 11600/137270 (8.5%)\n",
            "Scoring pairs: 11700/137270 (8.5%)\n",
            "Scoring pairs: 11800/137270 (8.6%)\n",
            "Scoring pairs: 11900/137270 (8.7%)\n",
            "Scoring pairs: 12000/137270 (8.7%)\n",
            "Scoring pairs: 12100/137270 (8.8%)\n",
            "Scoring pairs: 12200/137270 (8.9%)\n",
            "Scoring pairs: 12300/137270 (9.0%)\n",
            "Scoring pairs: 12400/137270 (9.0%)\n",
            "Scoring pairs: 12500/137270 (9.1%)\n",
            "Scoring pairs: 12600/137270 (9.2%)\n",
            "Scoring pairs: 12700/137270 (9.3%)\n",
            "Scoring pairs: 12800/137270 (9.3%)\n",
            "Scoring pairs: 12900/137270 (9.4%)\n",
            "Scoring pairs: 13000/137270 (9.5%)\n",
            "Scoring pairs: 13100/137270 (9.5%)\n",
            "Scoring pairs: 13200/137270 (9.6%)\n",
            "Scoring pairs: 13300/137270 (9.7%)\n",
            "Scoring pairs: 13400/137270 (9.8%)\n",
            "Scoring pairs: 13500/137270 (9.8%)\n",
            "Scoring pairs: 13600/137270 (9.9%)\n",
            "Scoring pairs: 13700/137270 (10.0%)\n",
            "Scoring pairs: 13800/137270 (10.1%)\n",
            "Scoring pairs: 13900/137270 (10.1%)\n",
            "Scoring pairs: 14000/137270 (10.2%)\n",
            "Scoring pairs: 14100/137270 (10.3%)\n",
            "Scoring pairs: 14200/137270 (10.3%)\n",
            "Scoring pairs: 14300/137270 (10.4%)\n",
            "Scoring pairs: 14400/137270 (10.5%)\n",
            "Scoring pairs: 14500/137270 (10.6%)\n",
            "Scoring pairs: 14600/137270 (10.6%)\n",
            "Scoring pairs: 14700/137270 (10.7%)\n",
            "Scoring pairs: 14800/137270 (10.8%)\n",
            "Scoring pairs: 14900/137270 (10.9%)\n",
            "Scoring pairs: 15000/137270 (10.9%)\n",
            "Scoring pairs: 15100/137270 (11.0%)\n",
            "Scoring pairs: 15200/137270 (11.1%)\n",
            "Scoring pairs: 15300/137270 (11.1%)\n",
            "Scoring pairs: 15400/137270 (11.2%)\n",
            "Scoring pairs: 15500/137270 (11.3%)\n",
            "Scoring pairs: 15600/137270 (11.4%)\n",
            "Scoring pairs: 15700/137270 (11.4%)\n",
            "Scoring pairs: 15800/137270 (11.5%)\n",
            "Scoring pairs: 15900/137270 (11.6%)\n",
            "Scoring pairs: 16000/137270 (11.7%)\n",
            "Scoring pairs: 16100/137270 (11.7%)\n",
            "Scoring pairs: 16200/137270 (11.8%)\n",
            "Scoring pairs: 16300/137270 (11.9%)\n",
            "Scoring pairs: 16400/137270 (11.9%)\n",
            "Scoring pairs: 16500/137270 (12.0%)\n",
            "Scoring pairs: 16600/137270 (12.1%)\n",
            "Scoring pairs: 16700/137270 (12.2%)\n",
            "Scoring pairs: 16800/137270 (12.2%)\n",
            "Scoring pairs: 16900/137270 (12.3%)\n",
            "Scoring pairs: 17000/137270 (12.4%)\n",
            "Scoring pairs: 17100/137270 (12.5%)\n",
            "Scoring pairs: 17200/137270 (12.5%)\n",
            "Scoring pairs: 17300/137270 (12.6%)\n",
            "Scoring pairs: 17400/137270 (12.7%)\n",
            "Scoring pairs: 17500/137270 (12.7%)\n",
            "Scoring pairs: 17600/137270 (12.8%)\n",
            "Scoring pairs: 17700/137270 (12.9%)\n",
            "Scoring pairs: 17800/137270 (13.0%)\n",
            "Scoring pairs: 17900/137270 (13.0%)\n",
            "Scoring pairs: 18000/137270 (13.1%)\n",
            "Scoring pairs: 18100/137270 (13.2%)\n",
            "Scoring pairs: 18200/137270 (13.3%)\n",
            "Scoring pairs: 18300/137270 (13.3%)\n",
            "Scoring pairs: 18400/137270 (13.4%)\n",
            "Scoring pairs: 18500/137270 (13.5%)\n",
            "Scoring pairs: 18600/137270 (13.5%)\n",
            "Scoring pairs: 18700/137270 (13.6%)\n",
            "Scoring pairs: 18800/137270 (13.7%)\n",
            "Scoring pairs: 18900/137270 (13.8%)\n",
            "Scoring pairs: 19000/137270 (13.8%)\n",
            "Scoring pairs: 19100/137270 (13.9%)\n",
            "Scoring pairs: 19200/137270 (14.0%)\n",
            "Scoring pairs: 19300/137270 (14.1%)\n",
            "Scoring pairs: 19400/137270 (14.1%)\n",
            "Scoring pairs: 19500/137270 (14.2%)\n",
            "Scoring pairs: 19600/137270 (14.3%)\n",
            "Scoring pairs: 19700/137270 (14.4%)\n",
            "Scoring pairs: 19800/137270 (14.4%)\n",
            "Scoring pairs: 19900/137270 (14.5%)\n",
            "Scoring pairs: 20000/137270 (14.6%)\n",
            "Scoring pairs: 20100/137270 (14.6%)\n",
            "Scoring pairs: 20200/137270 (14.7%)\n",
            "Scoring pairs: 20300/137270 (14.8%)\n",
            "Scoring pairs: 20400/137270 (14.9%)\n",
            "Scoring pairs: 20500/137270 (14.9%)\n",
            "Scoring pairs: 20600/137270 (15.0%)\n",
            "Scoring pairs: 20700/137270 (15.1%)\n",
            "Scoring pairs: 20800/137270 (15.2%)\n",
            "Scoring pairs: 20900/137270 (15.2%)\n",
            "Scoring pairs: 21000/137270 (15.3%)\n",
            "Scoring pairs: 21100/137270 (15.4%)\n",
            "Scoring pairs: 21200/137270 (15.4%)\n",
            "Scoring pairs: 21300/137270 (15.5%)\n",
            "Scoring pairs: 21400/137270 (15.6%)\n",
            "Scoring pairs: 21500/137270 (15.7%)\n",
            "Scoring pairs: 21600/137270 (15.7%)\n",
            "Scoring pairs: 21700/137270 (15.8%)\n",
            "Scoring pairs: 21800/137270 (15.9%)\n",
            "Scoring pairs: 21900/137270 (16.0%)\n",
            "Scoring pairs: 22000/137270 (16.0%)\n",
            "Scoring pairs: 22100/137270 (16.1%)\n",
            "Scoring pairs: 22200/137270 (16.2%)\n",
            "Scoring pairs: 22300/137270 (16.2%)\n",
            "Scoring pairs: 22400/137270 (16.3%)\n",
            "Scoring pairs: 22500/137270 (16.4%)\n",
            "Scoring pairs: 22600/137270 (16.5%)\n",
            "Scoring pairs: 22700/137270 (16.5%)\n",
            "Scoring pairs: 22800/137270 (16.6%)\n",
            "Scoring pairs: 22900/137270 (16.7%)\n",
            "Scoring pairs: 23000/137270 (16.8%)\n",
            "Scoring pairs: 23100/137270 (16.8%)\n",
            "Scoring pairs: 23200/137270 (16.9%)\n",
            "Scoring pairs: 23300/137270 (17.0%)\n",
            "Scoring pairs: 23400/137270 (17.0%)\n",
            "Scoring pairs: 23500/137270 (17.1%)\n",
            "Scoring pairs: 23600/137270 (17.2%)\n",
            "Scoring pairs: 23700/137270 (17.3%)\n",
            "Scoring pairs: 23800/137270 (17.3%)\n",
            "Scoring pairs: 23900/137270 (17.4%)\n",
            "Scoring pairs: 24000/137270 (17.5%)\n",
            "Scoring pairs: 24100/137270 (17.6%)\n",
            "Scoring pairs: 24200/137270 (17.6%)\n",
            "Scoring pairs: 24300/137270 (17.7%)\n",
            "Scoring pairs: 24400/137270 (17.8%)\n",
            "Scoring pairs: 24500/137270 (17.8%)\n",
            "Scoring pairs: 24600/137270 (17.9%)\n",
            "Scoring pairs: 24700/137270 (18.0%)\n",
            "Scoring pairs: 24800/137270 (18.1%)\n",
            "Scoring pairs: 24900/137270 (18.1%)\n",
            "Scoring pairs: 25000/137270 (18.2%)\n",
            "Scoring pairs: 25100/137270 (18.3%)\n",
            "Scoring pairs: 25200/137270 (18.4%)\n",
            "Scoring pairs: 25300/137270 (18.4%)\n",
            "Scoring pairs: 25400/137270 (18.5%)\n",
            "Scoring pairs: 25500/137270 (18.6%)\n",
            "Scoring pairs: 25600/137270 (18.6%)\n",
            "Scoring pairs: 25700/137270 (18.7%)\n",
            "Scoring pairs: 25800/137270 (18.8%)\n",
            "Scoring pairs: 25900/137270 (18.9%)\n",
            "Scoring pairs: 26000/137270 (18.9%)\n",
            "Scoring pairs: 26100/137270 (19.0%)\n",
            "Scoring pairs: 26200/137270 (19.1%)\n",
            "Scoring pairs: 26300/137270 (19.2%)\n",
            "Scoring pairs: 26400/137270 (19.2%)\n",
            "Scoring pairs: 26500/137270 (19.3%)\n",
            "Scoring pairs: 26600/137270 (19.4%)\n",
            "Scoring pairs: 26700/137270 (19.5%)\n",
            "Scoring pairs: 26800/137270 (19.5%)\n",
            "Scoring pairs: 26900/137270 (19.6%)\n",
            "Scoring pairs: 27000/137270 (19.7%)\n",
            "Scoring pairs: 27100/137270 (19.7%)\n",
            "Scoring pairs: 27200/137270 (19.8%)\n",
            "Scoring pairs: 27300/137270 (19.9%)\n",
            "Scoring pairs: 27400/137270 (20.0%)\n",
            "Scoring pairs: 27500/137270 (20.0%)\n",
            "Scoring pairs: 27600/137270 (20.1%)\n",
            "Scoring pairs: 27700/137270 (20.2%)\n",
            "Scoring pairs: 27800/137270 (20.3%)\n",
            "Scoring pairs: 27900/137270 (20.3%)\n",
            "Scoring pairs: 28000/137270 (20.4%)\n",
            "Scoring pairs: 28100/137270 (20.5%)\n",
            "Scoring pairs: 28200/137270 (20.5%)\n",
            "Scoring pairs: 28300/137270 (20.6%)\n",
            "Scoring pairs: 28400/137270 (20.7%)\n",
            "Scoring pairs: 28500/137270 (20.8%)\n",
            "Scoring pairs: 28600/137270 (20.8%)\n",
            "Scoring pairs: 28700/137270 (20.9%)\n",
            "Scoring pairs: 28800/137270 (21.0%)\n",
            "Scoring pairs: 28900/137270 (21.1%)\n",
            "Scoring pairs: 29000/137270 (21.1%)\n",
            "Scoring pairs: 29100/137270 (21.2%)\n",
            "Scoring pairs: 29200/137270 (21.3%)\n",
            "Scoring pairs: 29300/137270 (21.3%)\n",
            "Scoring pairs: 29400/137270 (21.4%)\n",
            "Scoring pairs: 29500/137270 (21.5%)\n",
            "Scoring pairs: 29600/137270 (21.6%)\n",
            "Scoring pairs: 29700/137270 (21.6%)\n",
            "Scoring pairs: 29800/137270 (21.7%)\n",
            "Scoring pairs: 29900/137270 (21.8%)\n",
            "Scoring pairs: 30000/137270 (21.9%)\n",
            "Scoring pairs: 30100/137270 (21.9%)\n",
            "Scoring pairs: 30200/137270 (22.0%)\n",
            "Scoring pairs: 30300/137270 (22.1%)\n",
            "Scoring pairs: 30400/137270 (22.1%)\n",
            "Scoring pairs: 30500/137270 (22.2%)\n",
            "Scoring pairs: 30600/137270 (22.3%)\n",
            "Scoring pairs: 30700/137270 (22.4%)\n",
            "Scoring pairs: 30800/137270 (22.4%)\n",
            "Scoring pairs: 30900/137270 (22.5%)\n",
            "Scoring pairs: 31000/137270 (22.6%)\n",
            "Scoring pairs: 31100/137270 (22.7%)\n",
            "Scoring pairs: 31200/137270 (22.7%)\n",
            "Scoring pairs: 31300/137270 (22.8%)\n",
            "Scoring pairs: 31400/137270 (22.9%)\n",
            "Scoring pairs: 31500/137270 (22.9%)\n",
            "Scoring pairs: 31600/137270 (23.0%)\n",
            "Scoring pairs: 31700/137270 (23.1%)\n",
            "Scoring pairs: 31800/137270 (23.2%)\n",
            "Scoring pairs: 31900/137270 (23.2%)\n",
            "Scoring pairs: 32000/137270 (23.3%)\n",
            "Scoring pairs: 32100/137270 (23.4%)\n",
            "Scoring pairs: 32200/137270 (23.5%)\n",
            "Scoring pairs: 32300/137270 (23.5%)\n",
            "Scoring pairs: 32400/137270 (23.6%)\n",
            "Scoring pairs: 32500/137270 (23.7%)\n",
            "Scoring pairs: 32600/137270 (23.7%)\n",
            "Scoring pairs: 32700/137270 (23.8%)\n",
            "Scoring pairs: 32800/137270 (23.9%)\n",
            "Scoring pairs: 32900/137270 (24.0%)\n",
            "Scoring pairs: 33000/137270 (24.0%)\n",
            "Scoring pairs: 33100/137270 (24.1%)\n",
            "Scoring pairs: 33200/137270 (24.2%)\n",
            "Scoring pairs: 33300/137270 (24.3%)\n",
            "Scoring pairs: 33400/137270 (24.3%)\n",
            "Scoring pairs: 33500/137270 (24.4%)\n",
            "Scoring pairs: 33600/137270 (24.5%)\n",
            "Scoring pairs: 33700/137270 (24.6%)\n",
            "Scoring pairs: 33800/137270 (24.6%)\n",
            "Scoring pairs: 33900/137270 (24.7%)\n",
            "Scoring pairs: 34000/137270 (24.8%)\n",
            "Scoring pairs: 34100/137270 (24.8%)\n",
            "Scoring pairs: 34200/137270 (24.9%)\n",
            "Scoring pairs: 34300/137270 (25.0%)\n",
            "Scoring pairs: 34400/137270 (25.1%)\n",
            "Scoring pairs: 34500/137270 (25.1%)\n",
            "Scoring pairs: 34600/137270 (25.2%)\n",
            "Scoring pairs: 34700/137270 (25.3%)\n",
            "Scoring pairs: 34800/137270 (25.4%)\n",
            "Scoring pairs: 34900/137270 (25.4%)\n",
            "Scoring pairs: 35000/137270 (25.5%)\n",
            "Scoring pairs: 35100/137270 (25.6%)\n",
            "Scoring pairs: 35200/137270 (25.6%)\n",
            "Scoring pairs: 35300/137270 (25.7%)\n",
            "Scoring pairs: 35400/137270 (25.8%)\n",
            "Scoring pairs: 35500/137270 (25.9%)\n",
            "Scoring pairs: 35600/137270 (25.9%)\n",
            "Scoring pairs: 35700/137270 (26.0%)\n",
            "Scoring pairs: 35800/137270 (26.1%)\n",
            "Scoring pairs: 35900/137270 (26.2%)\n",
            "Scoring pairs: 36000/137270 (26.2%)\n",
            "Scoring pairs: 36100/137270 (26.3%)\n",
            "Scoring pairs: 36200/137270 (26.4%)\n",
            "Scoring pairs: 36300/137270 (26.4%)\n",
            "Scoring pairs: 36400/137270 (26.5%)\n",
            "Scoring pairs: 36500/137270 (26.6%)\n",
            "Scoring pairs: 36600/137270 (26.7%)\n",
            "Scoring pairs: 36700/137270 (26.7%)\n",
            "Scoring pairs: 36800/137270 (26.8%)\n",
            "Scoring pairs: 36900/137270 (26.9%)\n",
            "Scoring pairs: 37000/137270 (27.0%)\n",
            "Scoring pairs: 37100/137270 (27.0%)\n",
            "Scoring pairs: 37200/137270 (27.1%)\n",
            "Scoring pairs: 37300/137270 (27.2%)\n",
            "Scoring pairs: 37400/137270 (27.2%)\n",
            "Scoring pairs: 37500/137270 (27.3%)\n",
            "Scoring pairs: 37600/137270 (27.4%)\n",
            "Scoring pairs: 37700/137270 (27.5%)\n",
            "Scoring pairs: 37800/137270 (27.5%)\n",
            "Scoring pairs: 37900/137270 (27.6%)\n",
            "Scoring pairs: 38000/137270 (27.7%)\n",
            "Scoring pairs: 38100/137270 (27.8%)\n",
            "Scoring pairs: 38200/137270 (27.8%)\n",
            "Scoring pairs: 38300/137270 (27.9%)\n",
            "Scoring pairs: 38400/137270 (28.0%)\n",
            "Scoring pairs: 38500/137270 (28.0%)\n",
            "Scoring pairs: 38600/137270 (28.1%)\n",
            "Scoring pairs: 38700/137270 (28.2%)\n",
            "Scoring pairs: 38800/137270 (28.3%)\n",
            "Scoring pairs: 38900/137270 (28.3%)\n",
            "Scoring pairs: 39000/137270 (28.4%)\n",
            "Scoring pairs: 39100/137270 (28.5%)\n",
            "Scoring pairs: 39200/137270 (28.6%)\n",
            "Scoring pairs: 39300/137270 (28.6%)\n",
            "Scoring pairs: 39400/137270 (28.7%)\n",
            "Scoring pairs: 39500/137270 (28.8%)\n",
            "Scoring pairs: 39600/137270 (28.8%)\n",
            "Scoring pairs: 39700/137270 (28.9%)\n",
            "Scoring pairs: 39800/137270 (29.0%)\n",
            "Scoring pairs: 39900/137270 (29.1%)\n",
            "Scoring pairs: 40000/137270 (29.1%)\n",
            "Scoring pairs: 40100/137270 (29.2%)\n",
            "Scoring pairs: 40200/137270 (29.3%)\n",
            "Scoring pairs: 40300/137270 (29.4%)\n",
            "Scoring pairs: 40400/137270 (29.4%)\n",
            "Scoring pairs: 40500/137270 (29.5%)\n",
            "Scoring pairs: 40600/137270 (29.6%)\n",
            "Scoring pairs: 40700/137270 (29.6%)\n",
            "Scoring pairs: 40800/137270 (29.7%)\n",
            "Scoring pairs: 40900/137270 (29.8%)\n",
            "Scoring pairs: 41000/137270 (29.9%)\n",
            "Scoring pairs: 41100/137270 (29.9%)\n",
            "Scoring pairs: 41200/137270 (30.0%)\n",
            "Scoring pairs: 41300/137270 (30.1%)\n",
            "Scoring pairs: 41400/137270 (30.2%)\n",
            "Scoring pairs: 41500/137270 (30.2%)\n",
            "Scoring pairs: 41600/137270 (30.3%)\n",
            "Scoring pairs: 41700/137270 (30.4%)\n",
            "Scoring pairs: 41800/137270 (30.5%)\n",
            "Scoring pairs: 41900/137270 (30.5%)\n",
            "Scoring pairs: 42000/137270 (30.6%)\n",
            "Scoring pairs: 42100/137270 (30.7%)\n",
            "Scoring pairs: 42200/137270 (30.7%)\n",
            "Scoring pairs: 42300/137270 (30.8%)\n",
            "Scoring pairs: 42400/137270 (30.9%)\n",
            "Scoring pairs: 42500/137270 (31.0%)\n",
            "Scoring pairs: 42600/137270 (31.0%)\n",
            "Scoring pairs: 42700/137270 (31.1%)\n",
            "Scoring pairs: 42800/137270 (31.2%)\n",
            "Scoring pairs: 42900/137270 (31.3%)\n",
            "Scoring pairs: 43000/137270 (31.3%)\n",
            "Scoring pairs: 43100/137270 (31.4%)\n",
            "Scoring pairs: 43200/137270 (31.5%)\n",
            "Scoring pairs: 43300/137270 (31.5%)\n",
            "Scoring pairs: 43400/137270 (31.6%)\n",
            "Scoring pairs: 43500/137270 (31.7%)\n",
            "Scoring pairs: 43600/137270 (31.8%)\n",
            "Scoring pairs: 43700/137270 (31.8%)\n",
            "Scoring pairs: 43800/137270 (31.9%)\n",
            "Scoring pairs: 43900/137270 (32.0%)\n",
            "Scoring pairs: 44000/137270 (32.1%)\n",
            "Scoring pairs: 44100/137270 (32.1%)\n",
            "Scoring pairs: 44200/137270 (32.2%)\n",
            "Scoring pairs: 44300/137270 (32.3%)\n",
            "Scoring pairs: 44400/137270 (32.3%)\n",
            "Scoring pairs: 44500/137270 (32.4%)\n",
            "Scoring pairs: 44600/137270 (32.5%)\n",
            "Scoring pairs: 44700/137270 (32.6%)\n",
            "Scoring pairs: 44800/137270 (32.6%)\n",
            "Scoring pairs: 44900/137270 (32.7%)\n",
            "Scoring pairs: 45000/137270 (32.8%)\n",
            "Scoring pairs: 45100/137270 (32.9%)\n",
            "Scoring pairs: 45200/137270 (32.9%)\n",
            "Scoring pairs: 45300/137270 (33.0%)\n",
            "Scoring pairs: 45400/137270 (33.1%)\n",
            "Scoring pairs: 45500/137270 (33.1%)\n",
            "Scoring pairs: 45600/137270 (33.2%)\n",
            "Scoring pairs: 45700/137270 (33.3%)\n",
            "Scoring pairs: 45800/137270 (33.4%)\n",
            "Scoring pairs: 45900/137270 (33.4%)\n",
            "Scoring pairs: 46000/137270 (33.5%)\n",
            "Scoring pairs: 46100/137270 (33.6%)\n",
            "Scoring pairs: 46200/137270 (33.7%)\n",
            "Scoring pairs: 46300/137270 (33.7%)\n",
            "Scoring pairs: 46400/137270 (33.8%)\n",
            "Scoring pairs: 46500/137270 (33.9%)\n",
            "Scoring pairs: 46600/137270 (33.9%)\n",
            "Scoring pairs: 46700/137270 (34.0%)\n",
            "Scoring pairs: 46800/137270 (34.1%)\n",
            "Scoring pairs: 46900/137270 (34.2%)\n",
            "Scoring pairs: 47000/137270 (34.2%)\n",
            "Scoring pairs: 47100/137270 (34.3%)\n",
            "Scoring pairs: 47200/137270 (34.4%)\n",
            "Scoring pairs: 47300/137270 (34.5%)\n",
            "Scoring pairs: 47400/137270 (34.5%)\n",
            "Scoring pairs: 47500/137270 (34.6%)\n",
            "Scoring pairs: 47600/137270 (34.7%)\n",
            "Scoring pairs: 47700/137270 (34.7%)\n",
            "Scoring pairs: 47800/137270 (34.8%)\n",
            "Scoring pairs: 47900/137270 (34.9%)\n",
            "Scoring pairs: 48000/137270 (35.0%)\n",
            "Scoring pairs: 48100/137270 (35.0%)\n",
            "Scoring pairs: 48200/137270 (35.1%)\n",
            "Scoring pairs: 48300/137270 (35.2%)\n",
            "Scoring pairs: 48400/137270 (35.3%)\n",
            "Scoring pairs: 48500/137270 (35.3%)\n",
            "Scoring pairs: 48600/137270 (35.4%)\n",
            "Scoring pairs: 48700/137270 (35.5%)\n",
            "Scoring pairs: 48800/137270 (35.6%)\n",
            "Scoring pairs: 48900/137270 (35.6%)\n",
            "Scoring pairs: 49000/137270 (35.7%)\n",
            "Scoring pairs: 49100/137270 (35.8%)\n",
            "Scoring pairs: 49200/137270 (35.8%)\n",
            "Scoring pairs: 49300/137270 (35.9%)\n",
            "Scoring pairs: 49400/137270 (36.0%)\n",
            "Scoring pairs: 49500/137270 (36.1%)\n",
            "Scoring pairs: 49600/137270 (36.1%)\n",
            "Scoring pairs: 49700/137270 (36.2%)\n",
            "Scoring pairs: 49800/137270 (36.3%)\n",
            "Scoring pairs: 49900/137270 (36.4%)\n",
            "Scoring pairs: 50000/137270 (36.4%)\n",
            "Scoring pairs: 50100/137270 (36.5%)\n",
            "Scoring pairs: 50200/137270 (36.6%)\n",
            "Scoring pairs: 50300/137270 (36.6%)\n",
            "Scoring pairs: 50400/137270 (36.7%)\n",
            "Scoring pairs: 50500/137270 (36.8%)\n",
            "Scoring pairs: 50600/137270 (36.9%)\n",
            "Scoring pairs: 50700/137270 (36.9%)\n",
            "Scoring pairs: 50800/137270 (37.0%)\n",
            "Scoring pairs: 50900/137270 (37.1%)\n",
            "Scoring pairs: 51000/137270 (37.2%)\n",
            "Scoring pairs: 51100/137270 (37.2%)\n",
            "Scoring pairs: 51200/137270 (37.3%)\n",
            "Scoring pairs: 51300/137270 (37.4%)\n",
            "Scoring pairs: 51400/137270 (37.4%)\n",
            "Scoring pairs: 51500/137270 (37.5%)\n",
            "Scoring pairs: 51600/137270 (37.6%)\n",
            "Scoring pairs: 51700/137270 (37.7%)\n",
            "Scoring pairs: 51800/137270 (37.7%)\n",
            "Scoring pairs: 51900/137270 (37.8%)\n",
            "Scoring pairs: 52000/137270 (37.9%)\n",
            "Scoring pairs: 52100/137270 (38.0%)\n",
            "Scoring pairs: 52200/137270 (38.0%)\n",
            "Scoring pairs: 52300/137270 (38.1%)\n",
            "Scoring pairs: 52400/137270 (38.2%)\n",
            "Scoring pairs: 52500/137270 (38.2%)\n",
            "Scoring pairs: 52600/137270 (38.3%)\n",
            "Scoring pairs: 52700/137270 (38.4%)\n",
            "Scoring pairs: 52800/137270 (38.5%)\n",
            "Scoring pairs: 52900/137270 (38.5%)\n",
            "Scoring pairs: 53000/137270 (38.6%)\n",
            "Scoring pairs: 53100/137270 (38.7%)\n",
            "Scoring pairs: 53200/137270 (38.8%)\n",
            "Scoring pairs: 53300/137270 (38.8%)\n",
            "Scoring pairs: 53400/137270 (38.9%)\n",
            "Scoring pairs: 53500/137270 (39.0%)\n",
            "Scoring pairs: 53600/137270 (39.0%)\n",
            "Scoring pairs: 53700/137270 (39.1%)\n",
            "Scoring pairs: 53800/137270 (39.2%)\n",
            "Scoring pairs: 53900/137270 (39.3%)\n",
            "Scoring pairs: 54000/137270 (39.3%)\n",
            "Scoring pairs: 54100/137270 (39.4%)\n",
            "Scoring pairs: 54200/137270 (39.5%)\n",
            "Scoring pairs: 54300/137270 (39.6%)\n",
            "Scoring pairs: 54400/137270 (39.6%)\n",
            "Scoring pairs: 54500/137270 (39.7%)\n",
            "Scoring pairs: 54600/137270 (39.8%)\n",
            "Scoring pairs: 54700/137270 (39.8%)\n",
            "Scoring pairs: 54800/137270 (39.9%)\n",
            "Scoring pairs: 54900/137270 (40.0%)\n",
            "Scoring pairs: 55000/137270 (40.1%)\n",
            "Scoring pairs: 55100/137270 (40.1%)\n",
            "Scoring pairs: 55200/137270 (40.2%)\n",
            "Scoring pairs: 55300/137270 (40.3%)\n",
            "Scoring pairs: 55400/137270 (40.4%)\n",
            "Scoring pairs: 55500/137270 (40.4%)\n",
            "Scoring pairs: 55600/137270 (40.5%)\n",
            "Scoring pairs: 55700/137270 (40.6%)\n",
            "Scoring pairs: 55800/137270 (40.6%)\n",
            "Scoring pairs: 55900/137270 (40.7%)\n",
            "Scoring pairs: 56000/137270 (40.8%)\n",
            "Scoring pairs: 56100/137270 (40.9%)\n",
            "Scoring pairs: 56200/137270 (40.9%)\n",
            "Scoring pairs: 56300/137270 (41.0%)\n",
            "Scoring pairs: 56400/137270 (41.1%)\n",
            "Scoring pairs: 56500/137270 (41.2%)\n",
            "Scoring pairs: 56600/137270 (41.2%)\n",
            "Scoring pairs: 56700/137270 (41.3%)\n",
            "Scoring pairs: 56800/137270 (41.4%)\n",
            "Scoring pairs: 56900/137270 (41.5%)\n",
            "Scoring pairs: 57000/137270 (41.5%)\n",
            "Scoring pairs: 57100/137270 (41.6%)\n",
            "Scoring pairs: 57200/137270 (41.7%)\n",
            "Scoring pairs: 57300/137270 (41.7%)\n",
            "Scoring pairs: 57400/137270 (41.8%)\n",
            "Scoring pairs: 57500/137270 (41.9%)\n",
            "Scoring pairs: 57600/137270 (42.0%)\n",
            "Scoring pairs: 57700/137270 (42.0%)\n",
            "Scoring pairs: 57800/137270 (42.1%)\n",
            "Scoring pairs: 57900/137270 (42.2%)\n",
            "Scoring pairs: 58000/137270 (42.3%)\n",
            "Scoring pairs: 58100/137270 (42.3%)\n",
            "Scoring pairs: 58200/137270 (42.4%)\n",
            "Scoring pairs: 58300/137270 (42.5%)\n",
            "Scoring pairs: 58400/137270 (42.5%)\n",
            "Scoring pairs: 58500/137270 (42.6%)\n",
            "Scoring pairs: 58600/137270 (42.7%)\n",
            "Scoring pairs: 58700/137270 (42.8%)\n",
            "Scoring pairs: 58800/137270 (42.8%)\n",
            "Scoring pairs: 58900/137270 (42.9%)\n",
            "Scoring pairs: 59000/137270 (43.0%)\n",
            "Scoring pairs: 59100/137270 (43.1%)\n",
            "Scoring pairs: 59200/137270 (43.1%)\n",
            "Scoring pairs: 59300/137270 (43.2%)\n",
            "Scoring pairs: 59400/137270 (43.3%)\n",
            "Scoring pairs: 59500/137270 (43.3%)\n",
            "Scoring pairs: 59600/137270 (43.4%)\n",
            "Scoring pairs: 59700/137270 (43.5%)\n",
            "Scoring pairs: 59800/137270 (43.6%)\n",
            "Scoring pairs: 59900/137270 (43.6%)\n",
            "Scoring pairs: 60000/137270 (43.7%)\n",
            "Scoring pairs: 60100/137270 (43.8%)\n",
            "Scoring pairs: 60200/137270 (43.9%)\n",
            "Scoring pairs: 60300/137270 (43.9%)\n",
            "Scoring pairs: 60400/137270 (44.0%)\n",
            "Scoring pairs: 60500/137270 (44.1%)\n",
            "Scoring pairs: 60600/137270 (44.1%)\n",
            "Scoring pairs: 60700/137270 (44.2%)\n",
            "Scoring pairs: 60800/137270 (44.3%)\n",
            "Scoring pairs: 60900/137270 (44.4%)\n",
            "Scoring pairs: 61000/137270 (44.4%)\n",
            "Scoring pairs: 61100/137270 (44.5%)\n",
            "Scoring pairs: 61200/137270 (44.6%)\n",
            "Scoring pairs: 61300/137270 (44.7%)\n",
            "Scoring pairs: 61400/137270 (44.7%)\n",
            "Scoring pairs: 61500/137270 (44.8%)\n",
            "Scoring pairs: 61600/137270 (44.9%)\n",
            "Scoring pairs: 61700/137270 (44.9%)\n",
            "Scoring pairs: 61800/137270 (45.0%)\n",
            "Scoring pairs: 61900/137270 (45.1%)\n",
            "Scoring pairs: 62000/137270 (45.2%)\n",
            "Scoring pairs: 62100/137270 (45.2%)\n",
            "Scoring pairs: 62200/137270 (45.3%)\n",
            "Scoring pairs: 62300/137270 (45.4%)\n",
            "Scoring pairs: 62400/137270 (45.5%)\n",
            "Scoring pairs: 62500/137270 (45.5%)\n",
            "Scoring pairs: 62600/137270 (45.6%)\n",
            "Scoring pairs: 62700/137270 (45.7%)\n",
            "Scoring pairs: 62800/137270 (45.7%)\n",
            "Scoring pairs: 62900/137270 (45.8%)\n",
            "Scoring pairs: 63000/137270 (45.9%)\n",
            "Scoring pairs: 63100/137270 (46.0%)\n",
            "Scoring pairs: 63200/137270 (46.0%)\n",
            "Scoring pairs: 63300/137270 (46.1%)\n",
            "Scoring pairs: 63400/137270 (46.2%)\n",
            "Scoring pairs: 63500/137270 (46.3%)\n",
            "Scoring pairs: 63600/137270 (46.3%)\n",
            "Scoring pairs: 63700/137270 (46.4%)\n",
            "Scoring pairs: 63800/137270 (46.5%)\n",
            "Scoring pairs: 63900/137270 (46.6%)\n",
            "Scoring pairs: 64000/137270 (46.6%)\n",
            "Scoring pairs: 64100/137270 (46.7%)\n",
            "Scoring pairs: 64200/137270 (46.8%)\n",
            "Scoring pairs: 64300/137270 (46.8%)\n",
            "Scoring pairs: 64400/137270 (46.9%)\n",
            "Scoring pairs: 64500/137270 (47.0%)\n",
            "Scoring pairs: 64600/137270 (47.1%)\n",
            "Scoring pairs: 64700/137270 (47.1%)\n",
            "Scoring pairs: 64800/137270 (47.2%)\n",
            "Scoring pairs: 64900/137270 (47.3%)\n",
            "Scoring pairs: 65000/137270 (47.4%)\n",
            "Scoring pairs: 65100/137270 (47.4%)\n",
            "Scoring pairs: 65200/137270 (47.5%)\n",
            "Scoring pairs: 65300/137270 (47.6%)\n",
            "Scoring pairs: 65400/137270 (47.6%)\n",
            "Scoring pairs: 65500/137270 (47.7%)\n",
            "Scoring pairs: 65600/137270 (47.8%)\n",
            "Scoring pairs: 65700/137270 (47.9%)\n",
            "Scoring pairs: 65800/137270 (47.9%)\n",
            "Scoring pairs: 65900/137270 (48.0%)\n",
            "Scoring pairs: 66000/137270 (48.1%)\n",
            "Scoring pairs: 66100/137270 (48.2%)\n",
            "Scoring pairs: 66200/137270 (48.2%)\n",
            "Scoring pairs: 66300/137270 (48.3%)\n",
            "Scoring pairs: 66400/137270 (48.4%)\n",
            "Scoring pairs: 66500/137270 (48.4%)\n",
            "Scoring pairs: 66600/137270 (48.5%)\n",
            "Scoring pairs: 66700/137270 (48.6%)\n",
            "Scoring pairs: 66800/137270 (48.7%)\n",
            "Scoring pairs: 66900/137270 (48.7%)\n",
            "Scoring pairs: 67000/137270 (48.8%)\n",
            "Scoring pairs: 67100/137270 (48.9%)\n",
            "Scoring pairs: 67200/137270 (49.0%)\n",
            "Scoring pairs: 67300/137270 (49.0%)\n",
            "Scoring pairs: 67400/137270 (49.1%)\n",
            "Scoring pairs: 67500/137270 (49.2%)\n",
            "Scoring pairs: 67600/137270 (49.2%)\n",
            "Scoring pairs: 67700/137270 (49.3%)\n",
            "Scoring pairs: 67800/137270 (49.4%)\n",
            "Scoring pairs: 67900/137270 (49.5%)\n",
            "Scoring pairs: 68000/137270 (49.5%)\n",
            "Scoring pairs: 68100/137270 (49.6%)\n",
            "Scoring pairs: 68200/137270 (49.7%)\n",
            "Scoring pairs: 68300/137270 (49.8%)\n",
            "Scoring pairs: 68400/137270 (49.8%)\n",
            "Scoring pairs: 68500/137270 (49.9%)\n",
            "Scoring pairs: 68600/137270 (50.0%)\n",
            "Scoring pairs: 68700/137270 (50.0%)\n",
            "Scoring pairs: 68800/137270 (50.1%)\n",
            "Scoring pairs: 68900/137270 (50.2%)\n",
            "Scoring pairs: 69000/137270 (50.3%)\n",
            "Scoring pairs: 69100/137270 (50.3%)\n",
            "Scoring pairs: 69200/137270 (50.4%)\n",
            "Scoring pairs: 69300/137270 (50.5%)\n",
            "Scoring pairs: 69400/137270 (50.6%)\n",
            "Scoring pairs: 69500/137270 (50.6%)\n",
            "Scoring pairs: 69600/137270 (50.7%)\n",
            "Scoring pairs: 69700/137270 (50.8%)\n",
            "Scoring pairs: 69800/137270 (50.8%)\n",
            "Scoring pairs: 69900/137270 (50.9%)\n",
            "Scoring pairs: 70000/137270 (51.0%)\n",
            "Scoring pairs: 70100/137270 (51.1%)\n",
            "Scoring pairs: 70200/137270 (51.1%)\n",
            "Scoring pairs: 70300/137270 (51.2%)\n",
            "Scoring pairs: 70400/137270 (51.3%)\n",
            "Scoring pairs: 70500/137270 (51.4%)\n",
            "Scoring pairs: 70600/137270 (51.4%)\n",
            "Scoring pairs: 70700/137270 (51.5%)\n",
            "Scoring pairs: 70800/137270 (51.6%)\n",
            "Scoring pairs: 70900/137270 (51.7%)\n",
            "Scoring pairs: 71000/137270 (51.7%)\n",
            "Scoring pairs: 71100/137270 (51.8%)\n",
            "Scoring pairs: 71200/137270 (51.9%)\n",
            "Scoring pairs: 71300/137270 (51.9%)\n",
            "Scoring pairs: 71400/137270 (52.0%)\n",
            "Scoring pairs: 71500/137270 (52.1%)\n",
            "Scoring pairs: 71600/137270 (52.2%)\n",
            "Scoring pairs: 71700/137270 (52.2%)\n",
            "Scoring pairs: 71800/137270 (52.3%)\n",
            "Scoring pairs: 71900/137270 (52.4%)\n",
            "Scoring pairs: 72000/137270 (52.5%)\n",
            "Scoring pairs: 72100/137270 (52.5%)\n",
            "Scoring pairs: 72200/137270 (52.6%)\n",
            "Scoring pairs: 72300/137270 (52.7%)\n",
            "Scoring pairs: 72400/137270 (52.7%)\n",
            "Scoring pairs: 72500/137270 (52.8%)\n",
            "Scoring pairs: 72600/137270 (52.9%)\n",
            "Scoring pairs: 72700/137270 (53.0%)\n",
            "Scoring pairs: 72800/137270 (53.0%)\n",
            "Scoring pairs: 72900/137270 (53.1%)\n",
            "Scoring pairs: 73000/137270 (53.2%)\n",
            "Scoring pairs: 73100/137270 (53.3%)\n",
            "Scoring pairs: 73200/137270 (53.3%)\n",
            "Scoring pairs: 73300/137270 (53.4%)\n",
            "Scoring pairs: 73400/137270 (53.5%)\n",
            "Scoring pairs: 73500/137270 (53.5%)\n",
            "Scoring pairs: 73600/137270 (53.6%)\n",
            "Scoring pairs: 73700/137270 (53.7%)\n",
            "Scoring pairs: 73800/137270 (53.8%)\n",
            "Scoring pairs: 73900/137270 (53.8%)\n",
            "Scoring pairs: 74000/137270 (53.9%)\n",
            "Scoring pairs: 74100/137270 (54.0%)\n",
            "Scoring pairs: 74200/137270 (54.1%)\n",
            "Scoring pairs: 74300/137270 (54.1%)\n",
            "Scoring pairs: 74400/137270 (54.2%)\n",
            "Scoring pairs: 74500/137270 (54.3%)\n",
            "Scoring pairs: 74600/137270 (54.3%)\n",
            "Scoring pairs: 74700/137270 (54.4%)\n",
            "Scoring pairs: 74800/137270 (54.5%)\n",
            "Scoring pairs: 74900/137270 (54.6%)\n",
            "Scoring pairs: 75000/137270 (54.6%)\n",
            "Scoring pairs: 75100/137270 (54.7%)\n",
            "Scoring pairs: 75200/137270 (54.8%)\n",
            "Scoring pairs: 75300/137270 (54.9%)\n",
            "Scoring pairs: 75400/137270 (54.9%)\n",
            "Scoring pairs: 75500/137270 (55.0%)\n",
            "Scoring pairs: 75600/137270 (55.1%)\n",
            "Scoring pairs: 75700/137270 (55.1%)\n",
            "Scoring pairs: 75800/137270 (55.2%)\n",
            "Scoring pairs: 75900/137270 (55.3%)\n",
            "Scoring pairs: 76000/137270 (55.4%)\n",
            "Scoring pairs: 76100/137270 (55.4%)\n",
            "Scoring pairs: 76200/137270 (55.5%)\n",
            "Scoring pairs: 76300/137270 (55.6%)\n",
            "Scoring pairs: 76400/137270 (55.7%)\n",
            "Scoring pairs: 76500/137270 (55.7%)\n",
            "Scoring pairs: 76600/137270 (55.8%)\n",
            "Scoring pairs: 76700/137270 (55.9%)\n",
            "Scoring pairs: 76800/137270 (55.9%)\n",
            "Scoring pairs: 76900/137270 (56.0%)\n",
            "Scoring pairs: 77000/137270 (56.1%)\n",
            "Scoring pairs: 77100/137270 (56.2%)\n",
            "Scoring pairs: 77200/137270 (56.2%)\n",
            "Scoring pairs: 77300/137270 (56.3%)\n",
            "Scoring pairs: 77400/137270 (56.4%)\n",
            "Scoring pairs: 77500/137270 (56.5%)\n",
            "Scoring pairs: 77600/137270 (56.5%)\n",
            "Scoring pairs: 77700/137270 (56.6%)\n",
            "Scoring pairs: 77800/137270 (56.7%)\n",
            "Scoring pairs: 77900/137270 (56.7%)\n",
            "Scoring pairs: 78000/137270 (56.8%)\n",
            "Scoring pairs: 78100/137270 (56.9%)\n",
            "Scoring pairs: 78200/137270 (57.0%)\n",
            "Scoring pairs: 78300/137270 (57.0%)\n",
            "Scoring pairs: 78400/137270 (57.1%)\n",
            "Scoring pairs: 78500/137270 (57.2%)\n",
            "Scoring pairs: 78600/137270 (57.3%)\n",
            "Scoring pairs: 78700/137270 (57.3%)\n",
            "Scoring pairs: 78800/137270 (57.4%)\n",
            "Scoring pairs: 78900/137270 (57.5%)\n",
            "Scoring pairs: 79000/137270 (57.6%)\n",
            "Scoring pairs: 79100/137270 (57.6%)\n",
            "Scoring pairs: 79200/137270 (57.7%)\n",
            "Scoring pairs: 79300/137270 (57.8%)\n",
            "Scoring pairs: 79400/137270 (57.8%)\n",
            "Scoring pairs: 79500/137270 (57.9%)\n",
            "Scoring pairs: 79600/137270 (58.0%)\n",
            "Scoring pairs: 79700/137270 (58.1%)\n",
            "Scoring pairs: 79800/137270 (58.1%)\n",
            "Scoring pairs: 79900/137270 (58.2%)\n",
            "Scoring pairs: 80000/137270 (58.3%)\n",
            "Scoring pairs: 80100/137270 (58.4%)\n",
            "Scoring pairs: 80200/137270 (58.4%)\n",
            "Scoring pairs: 80300/137270 (58.5%)\n",
            "Scoring pairs: 80400/137270 (58.6%)\n",
            "Scoring pairs: 80500/137270 (58.6%)\n",
            "Scoring pairs: 80600/137270 (58.7%)\n",
            "Scoring pairs: 80700/137270 (58.8%)\n",
            "Scoring pairs: 80800/137270 (58.9%)\n",
            "Scoring pairs: 80900/137270 (58.9%)\n",
            "Scoring pairs: 81000/137270 (59.0%)\n",
            "Scoring pairs: 81100/137270 (59.1%)\n",
            "Scoring pairs: 81200/137270 (59.2%)\n",
            "Scoring pairs: 81300/137270 (59.2%)\n",
            "Scoring pairs: 81400/137270 (59.3%)\n",
            "Scoring pairs: 81500/137270 (59.4%)\n",
            "Scoring pairs: 81600/137270 (59.4%)\n",
            "Scoring pairs: 81700/137270 (59.5%)\n",
            "Scoring pairs: 81800/137270 (59.6%)\n",
            "Scoring pairs: 81900/137270 (59.7%)\n",
            "Scoring pairs: 82000/137270 (59.7%)\n",
            "Scoring pairs: 82100/137270 (59.8%)\n",
            "Scoring pairs: 82200/137270 (59.9%)\n",
            "Scoring pairs: 82300/137270 (60.0%)\n",
            "Scoring pairs: 82400/137270 (60.0%)\n",
            "Scoring pairs: 82500/137270 (60.1%)\n",
            "Scoring pairs: 82600/137270 (60.2%)\n",
            "Scoring pairs: 82700/137270 (60.2%)\n",
            "Scoring pairs: 82800/137270 (60.3%)\n",
            "Scoring pairs: 82900/137270 (60.4%)\n",
            "Scoring pairs: 83000/137270 (60.5%)\n",
            "Scoring pairs: 83100/137270 (60.5%)\n",
            "Scoring pairs: 83200/137270 (60.6%)\n",
            "Scoring pairs: 83300/137270 (60.7%)\n",
            "Scoring pairs: 83400/137270 (60.8%)\n",
            "Scoring pairs: 83500/137270 (60.8%)\n",
            "Scoring pairs: 83600/137270 (60.9%)\n",
            "Scoring pairs: 83700/137270 (61.0%)\n",
            "Scoring pairs: 83800/137270 (61.0%)\n",
            "Scoring pairs: 83900/137270 (61.1%)\n",
            "Scoring pairs: 84000/137270 (61.2%)\n",
            "Scoring pairs: 84100/137270 (61.3%)\n",
            "Scoring pairs: 84200/137270 (61.3%)\n",
            "Scoring pairs: 84300/137270 (61.4%)\n",
            "Scoring pairs: 84400/137270 (61.5%)\n",
            "Scoring pairs: 84500/137270 (61.6%)\n",
            "Scoring pairs: 84600/137270 (61.6%)\n",
            "Scoring pairs: 84700/137270 (61.7%)\n",
            "Scoring pairs: 84800/137270 (61.8%)\n",
            "Scoring pairs: 84900/137270 (61.8%)\n",
            "Scoring pairs: 85000/137270 (61.9%)\n",
            "Scoring pairs: 85100/137270 (62.0%)\n",
            "Scoring pairs: 85200/137270 (62.1%)\n",
            "Scoring pairs: 85300/137270 (62.1%)\n",
            "Scoring pairs: 85400/137270 (62.2%)\n",
            "Scoring pairs: 85500/137270 (62.3%)\n",
            "Scoring pairs: 85600/137270 (62.4%)\n",
            "Scoring pairs: 85700/137270 (62.4%)\n",
            "Scoring pairs: 85800/137270 (62.5%)\n",
            "Scoring pairs: 85900/137270 (62.6%)\n",
            "Scoring pairs: 86000/137270 (62.7%)\n",
            "Scoring pairs: 86100/137270 (62.7%)\n",
            "Scoring pairs: 86200/137270 (62.8%)\n",
            "Scoring pairs: 86300/137270 (62.9%)\n",
            "Scoring pairs: 86400/137270 (62.9%)\n",
            "Scoring pairs: 86500/137270 (63.0%)\n",
            "Scoring pairs: 86600/137270 (63.1%)\n",
            "Scoring pairs: 86700/137270 (63.2%)\n",
            "Scoring pairs: 86800/137270 (63.2%)\n",
            "Scoring pairs: 86900/137270 (63.3%)\n",
            "Scoring pairs: 87000/137270 (63.4%)\n",
            "Scoring pairs: 87100/137270 (63.5%)\n",
            "Scoring pairs: 87200/137270 (63.5%)\n",
            "Scoring pairs: 87300/137270 (63.6%)\n",
            "Scoring pairs: 87400/137270 (63.7%)\n",
            "Scoring pairs: 87500/137270 (63.7%)\n",
            "Scoring pairs: 87600/137270 (63.8%)\n",
            "Scoring pairs: 87700/137270 (63.9%)\n",
            "Scoring pairs: 87800/137270 (64.0%)\n",
            "Scoring pairs: 87900/137270 (64.0%)\n",
            "Scoring pairs: 88000/137270 (64.1%)\n",
            "Scoring pairs: 88100/137270 (64.2%)\n",
            "Scoring pairs: 88200/137270 (64.3%)\n",
            "Scoring pairs: 88300/137270 (64.3%)\n",
            "Scoring pairs: 88400/137270 (64.4%)\n",
            "Scoring pairs: 88500/137270 (64.5%)\n",
            "Scoring pairs: 88600/137270 (64.5%)\n",
            "Scoring pairs: 88700/137270 (64.6%)\n",
            "Scoring pairs: 88800/137270 (64.7%)\n",
            "Scoring pairs: 88900/137270 (64.8%)\n",
            "Scoring pairs: 89000/137270 (64.8%)\n",
            "Scoring pairs: 89100/137270 (64.9%)\n",
            "Scoring pairs: 89200/137270 (65.0%)\n",
            "Scoring pairs: 89300/137270 (65.1%)\n",
            "Scoring pairs: 89400/137270 (65.1%)\n",
            "Scoring pairs: 89500/137270 (65.2%)\n",
            "Scoring pairs: 89600/137270 (65.3%)\n",
            "Scoring pairs: 89700/137270 (65.3%)\n",
            "Scoring pairs: 89800/137270 (65.4%)\n",
            "Scoring pairs: 89900/137270 (65.5%)\n",
            "Scoring pairs: 90000/137270 (65.6%)\n",
            "Scoring pairs: 90100/137270 (65.6%)\n",
            "Scoring pairs: 90200/137270 (65.7%)\n",
            "Scoring pairs: 90300/137270 (65.8%)\n",
            "Scoring pairs: 90400/137270 (65.9%)\n",
            "Scoring pairs: 90500/137270 (65.9%)\n",
            "Scoring pairs: 90600/137270 (66.0%)\n",
            "Scoring pairs: 90700/137270 (66.1%)\n",
            "Scoring pairs: 90800/137270 (66.1%)\n",
            "Scoring pairs: 90900/137270 (66.2%)\n",
            "Scoring pairs: 91000/137270 (66.3%)\n",
            "Scoring pairs: 91100/137270 (66.4%)\n",
            "Scoring pairs: 91200/137270 (66.4%)\n",
            "Scoring pairs: 91300/137270 (66.5%)\n",
            "Scoring pairs: 91400/137270 (66.6%)\n",
            "Scoring pairs: 91500/137270 (66.7%)\n",
            "Scoring pairs: 91600/137270 (66.7%)\n",
            "Scoring pairs: 91700/137270 (66.8%)\n",
            "Scoring pairs: 91800/137270 (66.9%)\n",
            "Scoring pairs: 91900/137270 (66.9%)\n",
            "Scoring pairs: 92000/137270 (67.0%)\n",
            "Scoring pairs: 92100/137270 (67.1%)\n",
            "Scoring pairs: 92200/137270 (67.2%)\n",
            "Scoring pairs: 92300/137270 (67.2%)\n",
            "Scoring pairs: 92400/137270 (67.3%)\n",
            "Scoring pairs: 92500/137270 (67.4%)\n",
            "Scoring pairs: 92600/137270 (67.5%)\n",
            "Scoring pairs: 92700/137270 (67.5%)\n",
            "Scoring pairs: 92800/137270 (67.6%)\n",
            "Scoring pairs: 92900/137270 (67.7%)\n",
            "Scoring pairs: 93000/137270 (67.7%)\n",
            "Scoring pairs: 93100/137270 (67.8%)\n",
            "Scoring pairs: 93200/137270 (67.9%)\n",
            "Scoring pairs: 93300/137270 (68.0%)\n",
            "Scoring pairs: 93400/137270 (68.0%)\n",
            "Scoring pairs: 93500/137270 (68.1%)\n",
            "Scoring pairs: 93600/137270 (68.2%)\n",
            "Scoring pairs: 93700/137270 (68.3%)\n",
            "Scoring pairs: 93800/137270 (68.3%)\n",
            "Scoring pairs: 93900/137270 (68.4%)\n",
            "Scoring pairs: 94000/137270 (68.5%)\n",
            "Scoring pairs: 94100/137270 (68.6%)\n",
            "Scoring pairs: 94200/137270 (68.6%)\n",
            "Scoring pairs: 94300/137270 (68.7%)\n",
            "Scoring pairs: 94400/137270 (68.8%)\n",
            "Scoring pairs: 94500/137270 (68.8%)\n",
            "Scoring pairs: 94600/137270 (68.9%)\n",
            "Scoring pairs: 94700/137270 (69.0%)\n",
            "Scoring pairs: 94800/137270 (69.1%)\n",
            "Scoring pairs: 94900/137270 (69.1%)\n",
            "Scoring pairs: 95000/137270 (69.2%)\n",
            "Scoring pairs: 95100/137270 (69.3%)\n",
            "Scoring pairs: 95200/137270 (69.4%)\n",
            "Scoring pairs: 95300/137270 (69.4%)\n",
            "Scoring pairs: 95400/137270 (69.5%)\n",
            "Scoring pairs: 95500/137270 (69.6%)\n",
            "Scoring pairs: 95600/137270 (69.6%)\n",
            "Scoring pairs: 95700/137270 (69.7%)\n",
            "Scoring pairs: 95800/137270 (69.8%)\n",
            "Scoring pairs: 95900/137270 (69.9%)\n",
            "Scoring pairs: 96000/137270 (69.9%)\n",
            "Scoring pairs: 96100/137270 (70.0%)\n",
            "Scoring pairs: 96200/137270 (70.1%)\n",
            "Scoring pairs: 96300/137270 (70.2%)\n",
            "Scoring pairs: 96400/137270 (70.2%)\n",
            "Scoring pairs: 96500/137270 (70.3%)\n",
            "Scoring pairs: 96600/137270 (70.4%)\n",
            "Scoring pairs: 96700/137270 (70.4%)\n",
            "Scoring pairs: 96800/137270 (70.5%)\n",
            "Scoring pairs: 96900/137270 (70.6%)\n",
            "Scoring pairs: 97000/137270 (70.7%)\n",
            "Scoring pairs: 97100/137270 (70.7%)\n",
            "Scoring pairs: 97200/137270 (70.8%)\n",
            "Scoring pairs: 97300/137270 (70.9%)\n",
            "Scoring pairs: 97400/137270 (71.0%)\n",
            "Scoring pairs: 97500/137270 (71.0%)\n",
            "Scoring pairs: 97600/137270 (71.1%)\n",
            "Scoring pairs: 97700/137270 (71.2%)\n",
            "Scoring pairs: 97800/137270 (71.2%)\n",
            "Scoring pairs: 97900/137270 (71.3%)\n",
            "Scoring pairs: 98000/137270 (71.4%)\n",
            "Scoring pairs: 98100/137270 (71.5%)\n",
            "Scoring pairs: 98200/137270 (71.5%)\n",
            "Scoring pairs: 98300/137270 (71.6%)\n",
            "Scoring pairs: 98400/137270 (71.7%)\n",
            "Scoring pairs: 98500/137270 (71.8%)\n",
            "Scoring pairs: 98600/137270 (71.8%)\n",
            "Scoring pairs: 98700/137270 (71.9%)\n",
            "Scoring pairs: 98800/137270 (72.0%)\n",
            "Scoring pairs: 98900/137270 (72.0%)\n",
            "Scoring pairs: 99000/137270 (72.1%)\n",
            "Scoring pairs: 99100/137270 (72.2%)\n",
            "Scoring pairs: 99200/137270 (72.3%)\n",
            "Scoring pairs: 99300/137270 (72.3%)\n",
            "Scoring pairs: 99400/137270 (72.4%)\n",
            "Scoring pairs: 99500/137270 (72.5%)\n",
            "Scoring pairs: 99600/137270 (72.6%)\n",
            "Scoring pairs: 99700/137270 (72.6%)\n",
            "Scoring pairs: 99800/137270 (72.7%)\n",
            "Scoring pairs: 99900/137270 (72.8%)\n",
            "Scoring pairs: 100000/137270 (72.8%)\n",
            "Scoring pairs: 100100/137270 (72.9%)\n",
            "Scoring pairs: 100200/137270 (73.0%)\n",
            "Scoring pairs: 100300/137270 (73.1%)\n",
            "Scoring pairs: 100400/137270 (73.1%)\n",
            "Scoring pairs: 100500/137270 (73.2%)\n",
            "Scoring pairs: 100600/137270 (73.3%)\n",
            "Scoring pairs: 100700/137270 (73.4%)\n",
            "Scoring pairs: 100800/137270 (73.4%)\n",
            "Scoring pairs: 100900/137270 (73.5%)\n",
            "Scoring pairs: 101000/137270 (73.6%)\n",
            "Scoring pairs: 101100/137270 (73.7%)\n",
            "Scoring pairs: 101200/137270 (73.7%)\n",
            "Scoring pairs: 101300/137270 (73.8%)\n",
            "Scoring pairs: 101400/137270 (73.9%)\n",
            "Scoring pairs: 101500/137270 (73.9%)\n",
            "Scoring pairs: 101600/137270 (74.0%)\n",
            "Scoring pairs: 101700/137270 (74.1%)\n",
            "Scoring pairs: 101800/137270 (74.2%)\n",
            "Scoring pairs: 101900/137270 (74.2%)\n",
            "Scoring pairs: 102000/137270 (74.3%)\n",
            "Scoring pairs: 102100/137270 (74.4%)\n",
            "Scoring pairs: 102200/137270 (74.5%)\n",
            "Scoring pairs: 102300/137270 (74.5%)\n",
            "Scoring pairs: 102400/137270 (74.6%)\n",
            "Scoring pairs: 102500/137270 (74.7%)\n",
            "Scoring pairs: 102600/137270 (74.7%)\n",
            "Scoring pairs: 102700/137270 (74.8%)\n",
            "Scoring pairs: 102800/137270 (74.9%)\n",
            "Scoring pairs: 102900/137270 (75.0%)\n",
            "Scoring pairs: 103000/137270 (75.0%)\n",
            "Scoring pairs: 103100/137270 (75.1%)\n",
            "Scoring pairs: 103200/137270 (75.2%)\n",
            "Scoring pairs: 103300/137270 (75.3%)\n",
            "Scoring pairs: 103400/137270 (75.3%)\n",
            "Scoring pairs: 103500/137270 (75.4%)\n",
            "Scoring pairs: 103600/137270 (75.5%)\n",
            "Scoring pairs: 103700/137270 (75.5%)\n",
            "Scoring pairs: 103800/137270 (75.6%)\n",
            "Scoring pairs: 103900/137270 (75.7%)\n",
            "Scoring pairs: 104000/137270 (75.8%)\n",
            "Scoring pairs: 104100/137270 (75.8%)\n",
            "Scoring pairs: 104200/137270 (75.9%)\n",
            "Scoring pairs: 104300/137270 (76.0%)\n",
            "Scoring pairs: 104400/137270 (76.1%)\n",
            "Scoring pairs: 104500/137270 (76.1%)\n",
            "Scoring pairs: 104600/137270 (76.2%)\n",
            "Scoring pairs: 104700/137270 (76.3%)\n",
            "Scoring pairs: 104800/137270 (76.3%)\n",
            "Scoring pairs: 104900/137270 (76.4%)\n",
            "Scoring pairs: 105000/137270 (76.5%)\n",
            "Scoring pairs: 105100/137270 (76.6%)\n",
            "Scoring pairs: 105200/137270 (76.6%)\n",
            "Scoring pairs: 105300/137270 (76.7%)\n",
            "Scoring pairs: 105400/137270 (76.8%)\n",
            "Scoring pairs: 105500/137270 (76.9%)\n",
            "Scoring pairs: 105600/137270 (76.9%)\n",
            "Scoring pairs: 105700/137270 (77.0%)\n",
            "Scoring pairs: 105800/137270 (77.1%)\n",
            "Scoring pairs: 105900/137270 (77.1%)\n",
            "Scoring pairs: 106000/137270 (77.2%)\n",
            "Scoring pairs: 106100/137270 (77.3%)\n",
            "Scoring pairs: 106200/137270 (77.4%)\n",
            "Scoring pairs: 106300/137270 (77.4%)\n",
            "Scoring pairs: 106400/137270 (77.5%)\n",
            "Scoring pairs: 106500/137270 (77.6%)\n",
            "Scoring pairs: 106600/137270 (77.7%)\n",
            "Scoring pairs: 106700/137270 (77.7%)\n",
            "Scoring pairs: 106800/137270 (77.8%)\n",
            "Scoring pairs: 106900/137270 (77.9%)\n",
            "Scoring pairs: 107000/137270 (77.9%)\n",
            "Scoring pairs: 107100/137270 (78.0%)\n",
            "Scoring pairs: 107200/137270 (78.1%)\n",
            "Scoring pairs: 107300/137270 (78.2%)\n",
            "Scoring pairs: 107400/137270 (78.2%)\n",
            "Scoring pairs: 107500/137270 (78.3%)\n",
            "Scoring pairs: 107600/137270 (78.4%)\n",
            "Scoring pairs: 107700/137270 (78.5%)\n",
            "Scoring pairs: 107800/137270 (78.5%)\n",
            "Scoring pairs: 107900/137270 (78.6%)\n",
            "Scoring pairs: 108000/137270 (78.7%)\n",
            "Scoring pairs: 108100/137270 (78.7%)\n",
            "Scoring pairs: 108200/137270 (78.8%)\n",
            "Scoring pairs: 108300/137270 (78.9%)\n",
            "Scoring pairs: 108400/137270 (79.0%)\n",
            "Scoring pairs: 108500/137270 (79.0%)\n",
            "Scoring pairs: 108600/137270 (79.1%)\n",
            "Scoring pairs: 108700/137270 (79.2%)\n",
            "Scoring pairs: 108800/137270 (79.3%)\n",
            "Scoring pairs: 108900/137270 (79.3%)\n",
            "Scoring pairs: 109000/137270 (79.4%)\n",
            "Scoring pairs: 109100/137270 (79.5%)\n",
            "Scoring pairs: 109200/137270 (79.6%)\n",
            "Scoring pairs: 109300/137270 (79.6%)\n",
            "Scoring pairs: 109400/137270 (79.7%)\n",
            "Scoring pairs: 109500/137270 (79.8%)\n",
            "Scoring pairs: 109600/137270 (79.8%)\n",
            "Scoring pairs: 109700/137270 (79.9%)\n",
            "Scoring pairs: 109800/137270 (80.0%)\n",
            "Scoring pairs: 109900/137270 (80.1%)\n",
            "Scoring pairs: 110000/137270 (80.1%)\n",
            "Scoring pairs: 110100/137270 (80.2%)\n",
            "Scoring pairs: 110200/137270 (80.3%)\n",
            "Scoring pairs: 110300/137270 (80.4%)\n",
            "Scoring pairs: 110400/137270 (80.4%)\n",
            "Scoring pairs: 110500/137270 (80.5%)\n",
            "Scoring pairs: 110600/137270 (80.6%)\n",
            "Scoring pairs: 110700/137270 (80.6%)\n",
            "Scoring pairs: 110800/137270 (80.7%)\n",
            "Scoring pairs: 110900/137270 (80.8%)\n",
            "Scoring pairs: 111000/137270 (80.9%)\n",
            "Scoring pairs: 111100/137270 (80.9%)\n",
            "Scoring pairs: 111200/137270 (81.0%)\n",
            "Scoring pairs: 111300/137270 (81.1%)\n",
            "Scoring pairs: 111400/137270 (81.2%)\n",
            "Scoring pairs: 111500/137270 (81.2%)\n",
            "Scoring pairs: 111600/137270 (81.3%)\n",
            "Scoring pairs: 111700/137270 (81.4%)\n",
            "Scoring pairs: 111800/137270 (81.4%)\n",
            "Scoring pairs: 111900/137270 (81.5%)\n",
            "Scoring pairs: 112000/137270 (81.6%)\n",
            "Scoring pairs: 112100/137270 (81.7%)\n",
            "Scoring pairs: 112200/137270 (81.7%)\n",
            "Scoring pairs: 112300/137270 (81.8%)\n",
            "Scoring pairs: 112400/137270 (81.9%)\n",
            "Scoring pairs: 112500/137270 (82.0%)\n",
            "Scoring pairs: 112600/137270 (82.0%)\n",
            "Scoring pairs: 112700/137270 (82.1%)\n",
            "Scoring pairs: 112800/137270 (82.2%)\n",
            "Scoring pairs: 112900/137270 (82.2%)\n",
            "Scoring pairs: 113000/137270 (82.3%)\n",
            "Scoring pairs: 113100/137270 (82.4%)\n",
            "Scoring pairs: 113200/137270 (82.5%)\n",
            "Scoring pairs: 113300/137270 (82.5%)\n",
            "Scoring pairs: 113400/137270 (82.6%)\n",
            "Scoring pairs: 113500/137270 (82.7%)\n",
            "Scoring pairs: 113600/137270 (82.8%)\n",
            "Scoring pairs: 113700/137270 (82.8%)\n",
            "Scoring pairs: 113800/137270 (82.9%)\n",
            "Scoring pairs: 113900/137270 (83.0%)\n",
            "Scoring pairs: 114000/137270 (83.0%)\n",
            "Scoring pairs: 114100/137270 (83.1%)\n",
            "Scoring pairs: 114200/137270 (83.2%)\n",
            "Scoring pairs: 114300/137270 (83.3%)\n",
            "Scoring pairs: 114400/137270 (83.3%)\n",
            "Scoring pairs: 114500/137270 (83.4%)\n",
            "Scoring pairs: 114600/137270 (83.5%)\n",
            "Scoring pairs: 114700/137270 (83.6%)\n",
            "Scoring pairs: 114800/137270 (83.6%)\n",
            "Scoring pairs: 114900/137270 (83.7%)\n",
            "Scoring pairs: 115000/137270 (83.8%)\n",
            "Scoring pairs: 115100/137270 (83.8%)\n",
            "Scoring pairs: 115200/137270 (83.9%)\n",
            "Scoring pairs: 115300/137270 (84.0%)\n",
            "Scoring pairs: 115400/137270 (84.1%)\n",
            "Scoring pairs: 115500/137270 (84.1%)\n",
            "Scoring pairs: 115600/137270 (84.2%)\n",
            "Scoring pairs: 115700/137270 (84.3%)\n",
            "Scoring pairs: 115800/137270 (84.4%)\n",
            "Scoring pairs: 115900/137270 (84.4%)\n",
            "Scoring pairs: 116000/137270 (84.5%)\n",
            "Scoring pairs: 116100/137270 (84.6%)\n",
            "Scoring pairs: 116200/137270 (84.7%)\n",
            "Scoring pairs: 116300/137270 (84.7%)\n",
            "Scoring pairs: 116400/137270 (84.8%)\n",
            "Scoring pairs: 116500/137270 (84.9%)\n",
            "Scoring pairs: 116600/137270 (84.9%)\n",
            "Scoring pairs: 116700/137270 (85.0%)\n",
            "Scoring pairs: 116800/137270 (85.1%)\n",
            "Scoring pairs: 116900/137270 (85.2%)\n",
            "Scoring pairs: 117000/137270 (85.2%)\n",
            "Scoring pairs: 117100/137270 (85.3%)\n",
            "Scoring pairs: 117200/137270 (85.4%)\n",
            "Scoring pairs: 117300/137270 (85.5%)\n",
            "Scoring pairs: 117400/137270 (85.5%)\n",
            "Scoring pairs: 117500/137270 (85.6%)\n",
            "Scoring pairs: 117600/137270 (85.7%)\n",
            "Scoring pairs: 117700/137270 (85.7%)\n",
            "Scoring pairs: 117800/137270 (85.8%)\n",
            "Scoring pairs: 117900/137270 (85.9%)\n",
            "Scoring pairs: 118000/137270 (86.0%)\n",
            "Scoring pairs: 118100/137270 (86.0%)\n",
            "Scoring pairs: 118200/137270 (86.1%)\n",
            "Scoring pairs: 118300/137270 (86.2%)\n",
            "Scoring pairs: 118400/137270 (86.3%)\n",
            "Scoring pairs: 118500/137270 (86.3%)\n",
            "Scoring pairs: 118600/137270 (86.4%)\n",
            "Scoring pairs: 118700/137270 (86.5%)\n",
            "Scoring pairs: 118800/137270 (86.5%)\n",
            "Scoring pairs: 118900/137270 (86.6%)\n",
            "Scoring pairs: 119000/137270 (86.7%)\n",
            "Scoring pairs: 119100/137270 (86.8%)\n",
            "Scoring pairs: 119200/137270 (86.8%)\n",
            "Scoring pairs: 119300/137270 (86.9%)\n",
            "Scoring pairs: 119400/137270 (87.0%)\n",
            "Scoring pairs: 119500/137270 (87.1%)\n",
            "Scoring pairs: 119600/137270 (87.1%)\n",
            "Scoring pairs: 119700/137270 (87.2%)\n",
            "Scoring pairs: 119800/137270 (87.3%)\n",
            "Scoring pairs: 119900/137270 (87.3%)\n",
            "Scoring pairs: 120000/137270 (87.4%)\n",
            "Scoring pairs: 120100/137270 (87.5%)\n",
            "Scoring pairs: 120200/137270 (87.6%)\n",
            "Scoring pairs: 120300/137270 (87.6%)\n",
            "Scoring pairs: 120400/137270 (87.7%)\n",
            "Scoring pairs: 120500/137270 (87.8%)\n",
            "Scoring pairs: 120600/137270 (87.9%)\n",
            "Scoring pairs: 120700/137270 (87.9%)\n",
            "Scoring pairs: 120800/137270 (88.0%)\n",
            "Scoring pairs: 120900/137270 (88.1%)\n",
            "Scoring pairs: 121000/137270 (88.1%)\n",
            "Scoring pairs: 121100/137270 (88.2%)\n",
            "Scoring pairs: 121200/137270 (88.3%)\n",
            "Scoring pairs: 121300/137270 (88.4%)\n",
            "Scoring pairs: 121400/137270 (88.4%)\n",
            "Scoring pairs: 121500/137270 (88.5%)\n",
            "Scoring pairs: 121600/137270 (88.6%)\n",
            "Scoring pairs: 121700/137270 (88.7%)\n",
            "Scoring pairs: 121800/137270 (88.7%)\n",
            "Scoring pairs: 121900/137270 (88.8%)\n",
            "Scoring pairs: 122000/137270 (88.9%)\n",
            "Scoring pairs: 122100/137270 (88.9%)\n",
            "Scoring pairs: 122200/137270 (89.0%)\n",
            "Scoring pairs: 122300/137270 (89.1%)\n",
            "Scoring pairs: 122400/137270 (89.2%)\n",
            "Scoring pairs: 122500/137270 (89.2%)\n",
            "Scoring pairs: 122600/137270 (89.3%)\n",
            "Scoring pairs: 122700/137270 (89.4%)\n",
            "Scoring pairs: 122800/137270 (89.5%)\n",
            "Scoring pairs: 122900/137270 (89.5%)\n",
            "Scoring pairs: 123000/137270 (89.6%)\n",
            "Scoring pairs: 123100/137270 (89.7%)\n",
            "Scoring pairs: 123200/137270 (89.8%)\n",
            "Scoring pairs: 123300/137270 (89.8%)\n",
            "Scoring pairs: 123400/137270 (89.9%)\n",
            "Scoring pairs: 123500/137270 (90.0%)\n",
            "Scoring pairs: 123600/137270 (90.0%)\n",
            "Scoring pairs: 123700/137270 (90.1%)\n",
            "Scoring pairs: 123800/137270 (90.2%)\n",
            "Scoring pairs: 123900/137270 (90.3%)\n",
            "Scoring pairs: 124000/137270 (90.3%)\n",
            "Scoring pairs: 124100/137270 (90.4%)\n",
            "Scoring pairs: 124200/137270 (90.5%)\n",
            "Scoring pairs: 124300/137270 (90.6%)\n",
            "Scoring pairs: 124400/137270 (90.6%)\n",
            "Scoring pairs: 124500/137270 (90.7%)\n",
            "Scoring pairs: 124600/137270 (90.8%)\n",
            "Scoring pairs: 124700/137270 (90.8%)\n",
            "Scoring pairs: 124800/137270 (90.9%)\n",
            "Scoring pairs: 124900/137270 (91.0%)\n",
            "Scoring pairs: 125000/137270 (91.1%)\n",
            "Scoring pairs: 125100/137270 (91.1%)\n",
            "Scoring pairs: 125200/137270 (91.2%)\n",
            "Scoring pairs: 125300/137270 (91.3%)\n",
            "Scoring pairs: 125400/137270 (91.4%)\n",
            "Scoring pairs: 125500/137270 (91.4%)\n",
            "Scoring pairs: 125600/137270 (91.5%)\n",
            "Scoring pairs: 125700/137270 (91.6%)\n",
            "Scoring pairs: 125800/137270 (91.6%)\n",
            "Scoring pairs: 125900/137270 (91.7%)\n",
            "Scoring pairs: 126000/137270 (91.8%)\n",
            "Scoring pairs: 126100/137270 (91.9%)\n",
            "Scoring pairs: 126200/137270 (91.9%)\n",
            "Scoring pairs: 126300/137270 (92.0%)\n",
            "Scoring pairs: 126400/137270 (92.1%)\n",
            "Scoring pairs: 126500/137270 (92.2%)\n",
            "Scoring pairs: 126600/137270 (92.2%)\n",
            "Scoring pairs: 126700/137270 (92.3%)\n",
            "Scoring pairs: 126800/137270 (92.4%)\n",
            "Scoring pairs: 126900/137270 (92.4%)\n",
            "Scoring pairs: 127000/137270 (92.5%)\n",
            "Scoring pairs: 127100/137270 (92.6%)\n",
            "Scoring pairs: 127200/137270 (92.7%)\n",
            "Scoring pairs: 127300/137270 (92.7%)\n",
            "Scoring pairs: 127400/137270 (92.8%)\n",
            "Scoring pairs: 127500/137270 (92.9%)\n",
            "Scoring pairs: 127600/137270 (93.0%)\n",
            "Scoring pairs: 127700/137270 (93.0%)\n",
            "Scoring pairs: 127800/137270 (93.1%)\n",
            "Scoring pairs: 127900/137270 (93.2%)\n",
            "Scoring pairs: 128000/137270 (93.2%)\n",
            "Scoring pairs: 128100/137270 (93.3%)\n",
            "Scoring pairs: 128200/137270 (93.4%)\n",
            "Scoring pairs: 128300/137270 (93.5%)\n",
            "Scoring pairs: 128400/137270 (93.5%)\n",
            "Scoring pairs: 128500/137270 (93.6%)\n",
            "Scoring pairs: 128600/137270 (93.7%)\n",
            "Scoring pairs: 128700/137270 (93.8%)\n",
            "Scoring pairs: 128800/137270 (93.8%)\n",
            "Scoring pairs: 128900/137270 (93.9%)\n",
            "Scoring pairs: 129000/137270 (94.0%)\n",
            "Scoring pairs: 129100/137270 (94.0%)\n",
            "Scoring pairs: 129200/137270 (94.1%)\n",
            "Scoring pairs: 129300/137270 (94.2%)\n",
            "Scoring pairs: 129400/137270 (94.3%)\n",
            "Scoring pairs: 129500/137270 (94.3%)\n",
            "Scoring pairs: 129600/137270 (94.4%)\n",
            "Scoring pairs: 129700/137270 (94.5%)\n",
            "Scoring pairs: 129800/137270 (94.6%)\n",
            "Scoring pairs: 129900/137270 (94.6%)\n",
            "Scoring pairs: 130000/137270 (94.7%)\n",
            "Scoring pairs: 130100/137270 (94.8%)\n",
            "Scoring pairs: 130200/137270 (94.8%)\n",
            "Scoring pairs: 130300/137270 (94.9%)\n",
            "Scoring pairs: 130400/137270 (95.0%)\n",
            "Scoring pairs: 130500/137270 (95.1%)\n",
            "Scoring pairs: 130600/137270 (95.1%)\n",
            "Scoring pairs: 130700/137270 (95.2%)\n",
            "Scoring pairs: 130800/137270 (95.3%)\n",
            "Scoring pairs: 130900/137270 (95.4%)\n",
            "Scoring pairs: 131000/137270 (95.4%)\n",
            "Scoring pairs: 131100/137270 (95.5%)\n",
            "Scoring pairs: 131200/137270 (95.6%)\n",
            "Scoring pairs: 131300/137270 (95.7%)\n",
            "Scoring pairs: 131400/137270 (95.7%)\n",
            "Scoring pairs: 131500/137270 (95.8%)\n",
            "Scoring pairs: 131600/137270 (95.9%)\n",
            "Scoring pairs: 131700/137270 (95.9%)\n",
            "Scoring pairs: 131800/137270 (96.0%)\n",
            "Scoring pairs: 131900/137270 (96.1%)\n",
            "Scoring pairs: 132000/137270 (96.2%)\n",
            "Scoring pairs: 132100/137270 (96.2%)\n",
            "Scoring pairs: 132200/137270 (96.3%)\n",
            "Scoring pairs: 132300/137270 (96.4%)\n",
            "Scoring pairs: 132400/137270 (96.5%)\n",
            "Scoring pairs: 132500/137270 (96.5%)\n",
            "Scoring pairs: 132600/137270 (96.6%)\n",
            "Scoring pairs: 132700/137270 (96.7%)\n",
            "Scoring pairs: 132800/137270 (96.7%)\n",
            "Scoring pairs: 132900/137270 (96.8%)\n",
            "Scoring pairs: 133000/137270 (96.9%)\n",
            "Scoring pairs: 133100/137270 (97.0%)\n",
            "Scoring pairs: 133200/137270 (97.0%)\n",
            "Scoring pairs: 133300/137270 (97.1%)\n",
            "Scoring pairs: 133400/137270 (97.2%)\n",
            "Scoring pairs: 133500/137270 (97.3%)\n",
            "Scoring pairs: 133600/137270 (97.3%)\n",
            "Scoring pairs: 133700/137270 (97.4%)\n",
            "Scoring pairs: 133800/137270 (97.5%)\n",
            "Scoring pairs: 133900/137270 (97.5%)\n",
            "Scoring pairs: 134000/137270 (97.6%)\n",
            "Scoring pairs: 134100/137270 (97.7%)\n",
            "Scoring pairs: 134200/137270 (97.8%)\n",
            "Scoring pairs: 134300/137270 (97.8%)\n",
            "Scoring pairs: 134400/137270 (97.9%)\n",
            "Scoring pairs: 134500/137270 (98.0%)\n",
            "Scoring pairs: 134600/137270 (98.1%)\n",
            "Scoring pairs: 134700/137270 (98.1%)\n",
            "Scoring pairs: 134800/137270 (98.2%)\n",
            "Scoring pairs: 134900/137270 (98.3%)\n",
            "Scoring pairs: 135000/137270 (98.3%)\n",
            "Scoring pairs: 135100/137270 (98.4%)\n",
            "Scoring pairs: 135200/137270 (98.5%)\n",
            "Scoring pairs: 135300/137270 (98.6%)\n",
            "Scoring pairs: 135400/137270 (98.6%)\n",
            "Scoring pairs: 135500/137270 (98.7%)\n",
            "Scoring pairs: 135600/137270 (98.8%)\n",
            "Scoring pairs: 135700/137270 (98.9%)\n",
            "Scoring pairs: 135800/137270 (98.9%)\n",
            "Scoring pairs: 135900/137270 (99.0%)\n",
            "Scoring pairs: 136000/137270 (99.1%)\n",
            "Scoring pairs: 136100/137270 (99.1%)\n",
            "Scoring pairs: 136200/137270 (99.2%)\n",
            "Scoring pairs: 136300/137270 (99.3%)\n",
            "Scoring pairs: 136400/137270 (99.4%)\n",
            "Scoring pairs: 136500/137270 (99.4%)\n",
            "Scoring pairs: 136600/137270 (99.5%)\n",
            "Scoring pairs: 136700/137270 (99.6%)\n",
            "Scoring pairs: 136800/137270 (99.7%)\n",
            "Scoring pairs: 136900/137270 (99.7%)\n",
            "Scoring pairs: 137000/137270 (99.8%)\n",
            "Scoring pairs: 137100/137270 (99.9%)\n",
            "Scoring pairs: 137200/137270 (99.9%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_dir = \"/content/drive/MyDrive/jaguar-re-id(1)/test/test\"\n",
        "test_csv = \"/content/drive/MyDrive/jaguar-re-id(1)/test.csv\"\n",
        "ckpt_path = \"/content/drive/MyDrive/jaguar-re-id(1)/ckpts/vit_fold0_best.pth\"  # or best fold\n",
        "\n",
        "out = make_submission(test_csv, test_dir, ckpt_path, out_path=\"submission.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Rx9uOP17mO0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}